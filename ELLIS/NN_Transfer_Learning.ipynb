{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN - Transfer Learning.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sam Ellis"
      ],
      "metadata": {
        "id": "jriV0c-dSpz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Prelims"
      ],
      "metadata": {
        "id": "aJPyp-gSSstz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnbtpmfzHXA3",
        "outputId": "81c4cc5b-f5bf-4068-df18-0d122e7fe004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "JfSqKHIKHs7M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "squeezenet = models.squeezenet1_0(pretrained=True)\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "densenet = models.densenet161(pretrained=True)\n",
        "inception = models.inception_v3(pretrained=True)\n",
        "googlenet = models.googlenet(pretrained=True)\n",
        "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
        "wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n",
        "mnasnet = models.mnasnet1_0(pretrained=True)\n",
        "\n",
        "models = [('googlenet', googlenet, []),('shufflenet', shufflenet, []),('mobilenet', mobilenet, []),('resnext50_32x4d', resnext50_32x4d, []),\n",
        "          ('wide_resnet50_2', wide_resnet50_2, []),('mnasnet', mnasnet, []), ('resnet18', resnet18, []),('alexnet', alexnet, []),\n",
        "          ('squeezenet', squeezenet, []),('vgg16', vgg16, []),('densenet', densenet, [])]"
      ],
      "metadata": {
        "id": "pE_lxIwrHv7T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If youve read my ResNets notebook, the same applies here. These variables need to point to the right places. "
      ],
      "metadata": {
        "id": "e9qHHgKpTHAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im_path = '/elpv-dataset'\n",
        "\n",
        "train_path = 'train_csv'\n",
        "test_path = 'test_csv'\n",
        "\n",
        "print(len(models))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZURSDJGIG6D",
        "outputId": "021d3607-9e18-4d0d-8f7c-75c2b8373088"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Dataset"
      ],
      "metadata": {
        "id": "SnqsY2wbQwGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quite similar to the dataset/loader in my ResNet notebook, however this time we stack the image on itself 3 times. This is a haphazard way of pumping grayscale images through CNN's trained on RBG images. "
      ],
      "metadata": {
        "id": "9a1MNW2tTU3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "class solar_cell_dataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None, rgb = True):\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.rbg = rgb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
        "        img = Image.open(img_name)\n",
        "\n",
        "        target = torch.tensor(self.labels.iloc[idx, 1:])\n",
        "        target = target.type(torch.FloatTensor)\n",
        "\n",
        "        if self.transform:\n",
        "          img = self.transform(img)\n",
        "        if self.rbg:\n",
        "          img = img.repeat(3, 1, 1)\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "bDwvFk6RQnKu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2) FC Linear Transfer Learner"
      ],
      "metadata": {
        "id": "5z7mpGQQQyzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very simple class below. Just a two layer linear network with a ReLu."
      ],
      "metadata": {
        "id": "CNTMwOtITomF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class transfer_learner(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(transfer_learner, self).__init__()\n",
        "    self.fc1 = nn.Linear(1000, 1000)\n",
        "    self.fc2 = nn.Linear(1000, 2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6ePKEnZvM1mu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3) Loading Models and Data"
      ],
      "metadata": {
        "id": "YLt2Sz8XQ-U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### HYPER PARAMS ###########\n",
        "batch_size = 64\n",
        "cores = 0 #-1\n",
        "lr = .001\n",
        "momentum = .6\n",
        "epochs = 100\n",
        "################################\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irqimZ2uQQ0c",
        "outputId": "f0b57575-b56a-484c-8da8-21f10e594fab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import RandomApply\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.RandomApply(nn.ModuleList([T.RandomAffine(degrees = (0, 30), translate = (0.1, 0.2), scale = (0.5, 1))]), p=.2)\n",
        "])\n",
        "\n",
        "s = solar_cell_dataset(csv_file=train_path, img_dir=im_path, transform=ToTensor())\n",
        "t = solar_cell_dataset(csv_file=test_path, img_dir=im_path, transform=ToTensor())\n",
        "\n",
        "train_loader = DataLoader(s,\n",
        "                          batch_size=batch_size,\n",
        "                          num_workers=cores)\n",
        "test_loader = DataLoader(t,\n",
        "                         batch_size=batch_size,\n",
        "                         num_workers=cores)"
      ],
      "metadata": {
        "id": "0vqoF4XxQ8it"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(s.__getitem__(0)[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvQdRa0lXUuw",
        "outputId": "07fa6461-42c3-43a3-a621-170458eb6ee7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 300, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "s_ = nn.Softmax(dim=1)\n",
        "def classify(x):\n",
        "  x = s_(x)\n",
        "  x = torch.argmax(x, dim=1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "LYhMN4oDRdmi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop works like this: get a model, stack the neural net on top of it, freeze the gradient of the cnn, train the NN and test every 9 epochs. During all this I record the best test accuracy. To generate the table in our report, I ran this twice. \n",
        "\n",
        "The first time I transformed the training data with ToTensor(), and the second time I used my transforms() above which applies a random affine transormation with probability .2."
      ],
      "metadata": {
        "id": "0wsFSM5qYtiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for name, model, test_acc in models:\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "  transfer = transfer_learner()\n",
        "\n",
        "  net = nn.Sequential(model, transfer)\n",
        "  net.to(device)\n",
        "  optimizer = optim.SGD(net.parameters(), lr = lr, momentum = momentum)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    t_loss = 0\n",
        "    acc = 0\n",
        "    for idx, data in enumerate(train_loader):\n",
        "      img, target = data\n",
        "      img, target = img.to(device), target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = net(img)\n",
        "      loss = loss_criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      t_loss += loss.item()\n",
        "      acc += (classify(output) == torch.argmax(target, dim=1)).float().sum()\n",
        "    accuracy = 100 * acc / (len(s))\n",
        "    print(f'({name}), [{epoch}], loss : {t_loss:.3f}, accuracy : {accuracy:.3f}%')\n",
        "\n",
        "    if (epoch + 1) % 9 == 0:\n",
        "      acc = 0\n",
        "      for idx, data in enumerate(test_loader):\n",
        "        img, target = data\n",
        "        img, target = img.to(device), target.to(device)\n",
        "\n",
        "        output = net(img)\n",
        "        acc += (classify(output) == torch.argmax(target, dim=1)).float().sum()\n",
        "      accuracy = 100 * acc / (len(t))\n",
        "      test_acc.append(accuracy)   \n",
        "      print('======================================================')\n",
        "      print(f'[{epoch}], TEST accuracy : {accuracy:.3f}%')\n",
        "      print('======================================================')\n",
        "  print(torch.cuda.memory_allocated())\n",
        "  del net\n",
        "  print(torch.cuda.memory_allocated())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTrdVGOlRidD",
        "outputId": "f1d94898-13a8-461f-8b89-370993086e5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(googlenet), [0], loss : 18.051, accuracy : 61.474%\n",
            "(googlenet), [1], loss : 17.919, accuracy : 66.817%\n",
            "(googlenet), [2], loss : 17.760, accuracy : 67.885%\n",
            "(googlenet), [3], loss : 17.576, accuracy : 68.391%\n",
            "(googlenet), [4], loss : 17.360, accuracy : 69.010%\n",
            "(googlenet), [5], loss : 17.158, accuracy : 69.460%\n",
            "(googlenet), [6], loss : 16.938, accuracy : 69.966%\n",
            "(googlenet), [7], loss : 16.868, accuracy : 70.360%\n",
            "(googlenet), [8], loss : 16.636, accuracy : 70.247%\n",
            "======================================================\n",
            "[8], TEST accuracy : 73.708%\n",
            "======================================================\n",
            "(googlenet), [9], loss : 16.649, accuracy : 71.035%\n",
            "(googlenet), [10], loss : 16.486, accuracy : 71.822%\n",
            "(googlenet), [11], loss : 16.376, accuracy : 71.822%\n",
            "(googlenet), [12], loss : 16.093, accuracy : 72.778%\n",
            "(googlenet), [13], loss : 16.139, accuracy : 72.778%\n",
            "(googlenet), [14], loss : 16.046, accuracy : 73.285%\n",
            "(googlenet), [15], loss : 15.840, accuracy : 74.016%\n",
            "(googlenet), [16], loss : 15.871, accuracy : 73.228%\n",
            "(googlenet), [17], loss : 15.710, accuracy : 74.128%\n",
            "======================================================\n",
            "[17], TEST accuracy : 77.753%\n",
            "======================================================\n",
            "(googlenet), [18], loss : 15.728, accuracy : 74.241%\n",
            "(googlenet), [19], loss : 15.565, accuracy : 74.522%\n",
            "(googlenet), [20], loss : 15.359, accuracy : 74.803%\n",
            "(googlenet), [21], loss : 15.363, accuracy : 74.522%\n",
            "(googlenet), [22], loss : 15.201, accuracy : 75.422%\n",
            "(googlenet), [23], loss : 15.328, accuracy : 75.253%\n",
            "(googlenet), [24], loss : 15.242, accuracy : 75.478%\n",
            "(googlenet), [25], loss : 15.055, accuracy : 75.028%\n",
            "(googlenet), [26], loss : 14.989, accuracy : 75.872%\n",
            "======================================================\n",
            "[26], TEST accuracy : 80.674%\n",
            "======================================================\n",
            "(googlenet), [27], loss : 15.031, accuracy : 76.659%\n",
            "(googlenet), [28], loss : 14.925, accuracy : 76.153%\n",
            "(googlenet), [29], loss : 14.816, accuracy : 76.378%\n",
            "(googlenet), [30], loss : 14.692, accuracy : 76.828%\n",
            "(googlenet), [31], loss : 14.645, accuracy : 76.772%\n",
            "(googlenet), [32], loss : 14.497, accuracy : 77.278%\n",
            "(googlenet), [33], loss : 14.634, accuracy : 77.278%\n",
            "(googlenet), [34], loss : 14.486, accuracy : 76.659%\n",
            "(googlenet), [35], loss : 14.369, accuracy : 77.278%\n",
            "======================================================\n",
            "[35], TEST accuracy : 81.573%\n",
            "======================================================\n",
            "(googlenet), [36], loss : 14.250, accuracy : 77.334%\n",
            "(googlenet), [37], loss : 14.208, accuracy : 78.009%\n",
            "(googlenet), [38], loss : 14.223, accuracy : 77.222%\n",
            "(googlenet), [39], loss : 14.167, accuracy : 77.897%\n",
            "(googlenet), [40], loss : 14.133, accuracy : 76.828%\n",
            "(googlenet), [41], loss : 14.014, accuracy : 78.459%\n",
            "(googlenet), [42], loss : 13.989, accuracy : 77.897%\n",
            "(googlenet), [43], loss : 13.899, accuracy : 78.346%\n",
            "(googlenet), [44], loss : 13.792, accuracy : 79.190%\n",
            "======================================================\n",
            "[44], TEST accuracy : 83.146%\n",
            "======================================================\n",
            "(googlenet), [45], loss : 13.735, accuracy : 78.515%\n",
            "(googlenet), [46], loss : 13.734, accuracy : 78.065%\n",
            "(googlenet), [47], loss : 13.712, accuracy : 78.178%\n",
            "(googlenet), [48], loss : 13.579, accuracy : 79.528%\n",
            "(googlenet), [49], loss : 13.572, accuracy : 79.021%\n",
            "(googlenet), [50], loss : 13.629, accuracy : 78.965%\n",
            "(googlenet), [51], loss : 13.442, accuracy : 78.571%\n",
            "(googlenet), [52], loss : 13.381, accuracy : 78.403%\n",
            "(googlenet), [53], loss : 13.503, accuracy : 79.021%\n",
            "======================================================\n",
            "[53], TEST accuracy : 82.697%\n",
            "======================================================\n",
            "(googlenet), [54], loss : 13.224, accuracy : 78.515%\n",
            "(googlenet), [55], loss : 13.125, accuracy : 79.696%\n",
            "(googlenet), [56], loss : 13.011, accuracy : 79.471%\n",
            "(googlenet), [57], loss : 12.882, accuracy : 80.034%\n",
            "(googlenet), [58], loss : 12.967, accuracy : 79.640%\n",
            "(googlenet), [59], loss : 12.866, accuracy : 80.821%\n",
            "(googlenet), [60], loss : 12.793, accuracy : 80.709%\n",
            "(googlenet), [61], loss : 12.942, accuracy : 79.415%\n",
            "(googlenet), [62], loss : 12.603, accuracy : 80.877%\n",
            "======================================================\n",
            "[62], TEST accuracy : 82.921%\n",
            "======================================================\n",
            "(googlenet), [63], loss : 12.685, accuracy : 79.584%\n",
            "(googlenet), [64], loss : 12.831, accuracy : 80.034%\n",
            "(googlenet), [65], loss : 12.555, accuracy : 81.327%\n",
            "(googlenet), [66], loss : 12.790, accuracy : 79.696%\n",
            "(googlenet), [67], loss : 12.586, accuracy : 80.540%\n",
            "(googlenet), [68], loss : 12.477, accuracy : 80.821%\n",
            "(googlenet), [69], loss : 12.313, accuracy : 81.834%\n",
            "(googlenet), [70], loss : 12.237, accuracy : 81.890%\n",
            "(googlenet), [71], loss : 12.180, accuracy : 82.283%\n",
            "======================================================\n",
            "[71], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(googlenet), [72], loss : 12.403, accuracy : 80.877%\n",
            "(googlenet), [73], loss : 12.162, accuracy : 80.765%\n",
            "(googlenet), [74], loss : 11.920, accuracy : 82.002%\n",
            "(googlenet), [75], loss : 11.986, accuracy : 82.058%\n",
            "(googlenet), [76], loss : 12.007, accuracy : 81.496%\n",
            "(googlenet), [77], loss : 11.872, accuracy : 82.283%\n",
            "(googlenet), [78], loss : 11.969, accuracy : 81.552%\n",
            "(googlenet), [79], loss : 11.794, accuracy : 81.665%\n",
            "(googlenet), [80], loss : 11.735, accuracy : 82.508%\n",
            "======================================================\n",
            "[80], TEST accuracy : 84.494%\n",
            "======================================================\n",
            "(googlenet), [81], loss : 11.869, accuracy : 81.834%\n",
            "(googlenet), [82], loss : 11.812, accuracy : 82.058%\n",
            "(googlenet), [83], loss : 11.640, accuracy : 82.283%\n",
            "(googlenet), [84], loss : 11.479, accuracy : 82.452%\n",
            "(googlenet), [85], loss : 11.522, accuracy : 83.240%\n",
            "(googlenet), [86], loss : 11.367, accuracy : 82.452%\n",
            "(googlenet), [87], loss : 11.533, accuracy : 83.408%\n",
            "(googlenet), [88], loss : 11.422, accuracy : 83.633%\n",
            "(googlenet), [89], loss : 11.346, accuracy : 82.790%\n",
            "======================================================\n",
            "[89], TEST accuracy : 80.674%\n",
            "======================================================\n",
            "(googlenet), [90], loss : 11.389, accuracy : 83.183%\n",
            "(googlenet), [91], loss : 11.157, accuracy : 83.183%\n",
            "(googlenet), [92], loss : 11.127, accuracy : 83.015%\n",
            "(googlenet), [93], loss : 10.870, accuracy : 84.083%\n",
            "(googlenet), [94], loss : 10.993, accuracy : 83.521%\n",
            "(googlenet), [95], loss : 10.910, accuracy : 84.196%\n",
            "(googlenet), [96], loss : 10.934, accuracy : 83.802%\n",
            "(googlenet), [97], loss : 10.733, accuracy : 84.139%\n",
            "(googlenet), [98], loss : 10.726, accuracy : 84.927%\n",
            "======================================================\n",
            "[98], TEST accuracy : 84.270%\n",
            "======================================================\n",
            "(googlenet), [99], loss : 10.635, accuracy : 83.971%\n",
            "161889792\n",
            "161889792\n",
            "(shufflenet), [0], loss : 16.983, accuracy : 66.029%\n",
            "(shufflenet), [1], loss : 17.543, accuracy : 67.379%\n",
            "(shufflenet), [2], loss : 16.997, accuracy : 69.460%\n",
            "(shufflenet), [3], loss : 16.578, accuracy : 70.529%\n",
            "(shufflenet), [4], loss : 16.225, accuracy : 71.710%\n",
            "(shufflenet), [5], loss : 15.912, accuracy : 72.103%\n",
            "(shufflenet), [6], loss : 15.625, accuracy : 72.835%\n",
            "(shufflenet), [7], loss : 15.356, accuracy : 73.566%\n",
            "(shufflenet), [8], loss : 15.101, accuracy : 74.184%\n",
            "======================================================\n",
            "[8], TEST accuracy : 77.079%\n",
            "======================================================\n",
            "(shufflenet), [9], loss : 14.858, accuracy : 75.028%\n",
            "(shufflenet), [10], loss : 14.623, accuracy : 75.759%\n",
            "(shufflenet), [11], loss : 14.396, accuracy : 76.322%\n",
            "(shufflenet), [12], loss : 14.175, accuracy : 77.503%\n",
            "(shufflenet), [13], loss : 13.959, accuracy : 78.065%\n",
            "(shufflenet), [14], loss : 13.749, accuracy : 78.403%\n",
            "(shufflenet), [15], loss : 13.542, accuracy : 78.965%\n",
            "(shufflenet), [16], loss : 13.339, accuracy : 79.528%\n",
            "(shufflenet), [17], loss : 13.139, accuracy : 79.696%\n",
            "======================================================\n",
            "[17], TEST accuracy : 80.225%\n",
            "======================================================\n",
            "(shufflenet), [18], loss : 12.941, accuracy : 79.921%\n",
            "(shufflenet), [19], loss : 12.747, accuracy : 80.259%\n",
            "(shufflenet), [20], loss : 12.554, accuracy : 80.540%\n",
            "(shufflenet), [21], loss : 12.364, accuracy : 80.990%\n",
            "(shufflenet), [22], loss : 12.177, accuracy : 81.271%\n",
            "(shufflenet), [23], loss : 11.991, accuracy : 81.777%\n",
            "(shufflenet), [24], loss : 11.807, accuracy : 82.283%\n",
            "(shufflenet), [25], loss : 11.625, accuracy : 82.508%\n",
            "(shufflenet), [26], loss : 11.445, accuracy : 83.183%\n",
            "======================================================\n",
            "[26], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(shufflenet), [27], loss : 11.267, accuracy : 83.577%\n",
            "(shufflenet), [28], loss : 11.090, accuracy : 83.858%\n",
            "(shufflenet), [29], loss : 10.915, accuracy : 84.139%\n",
            "(shufflenet), [30], loss : 10.742, accuracy : 84.758%\n",
            "(shufflenet), [31], loss : 10.571, accuracy : 85.096%\n",
            "(shufflenet), [32], loss : 10.400, accuracy : 85.602%\n",
            "(shufflenet), [33], loss : 10.232, accuracy : 85.883%\n",
            "(shufflenet), [34], loss : 10.065, accuracy : 85.996%\n",
            "(shufflenet), [35], loss : 9.900, accuracy : 86.727%\n",
            "======================================================\n",
            "[35], TEST accuracy : 82.247%\n",
            "======================================================\n",
            "(shufflenet), [36], loss : 9.736, accuracy : 87.008%\n",
            "(shufflenet), [37], loss : 9.574, accuracy : 87.514%\n",
            "(shufflenet), [38], loss : 9.414, accuracy : 87.852%\n",
            "(shufflenet), [39], loss : 9.256, accuracy : 88.189%\n",
            "(shufflenet), [40], loss : 9.099, accuracy : 88.864%\n",
            "(shufflenet), [41], loss : 8.944, accuracy : 89.089%\n",
            "(shufflenet), [42], loss : 8.791, accuracy : 89.539%\n",
            "(shufflenet), [43], loss : 8.639, accuracy : 90.157%\n",
            "(shufflenet), [44], loss : 8.490, accuracy : 90.382%\n",
            "======================================================\n",
            "[44], TEST accuracy : 83.146%\n",
            "======================================================\n",
            "(shufflenet), [45], loss : 8.342, accuracy : 90.607%\n",
            "(shufflenet), [46], loss : 8.196, accuracy : 90.945%\n",
            "(shufflenet), [47], loss : 8.052, accuracy : 91.282%\n",
            "(shufflenet), [48], loss : 7.909, accuracy : 91.732%\n",
            "(shufflenet), [49], loss : 7.769, accuracy : 92.014%\n",
            "(shufflenet), [50], loss : 7.630, accuracy : 92.351%\n",
            "(shufflenet), [51], loss : 7.494, accuracy : 92.520%\n",
            "(shufflenet), [52], loss : 7.359, accuracy : 92.801%\n",
            "(shufflenet), [53], loss : 7.227, accuracy : 92.913%\n",
            "======================================================\n",
            "[53], TEST accuracy : 83.371%\n",
            "======================================================\n",
            "(shufflenet), [54], loss : 7.095, accuracy : 93.082%\n",
            "(shufflenet), [55], loss : 6.967, accuracy : 93.307%\n",
            "(shufflenet), [56], loss : 6.840, accuracy : 93.476%\n",
            "(shufflenet), [57], loss : 6.714, accuracy : 93.645%\n",
            "(shufflenet), [58], loss : 6.591, accuracy : 93.813%\n",
            "(shufflenet), [59], loss : 6.470, accuracy : 93.982%\n",
            "(shufflenet), [60], loss : 6.350, accuracy : 94.376%\n",
            "(shufflenet), [61], loss : 6.233, accuracy : 94.488%\n",
            "(shufflenet), [62], loss : 6.117, accuracy : 94.601%\n",
            "======================================================\n",
            "[62], TEST accuracy : 82.921%\n",
            "======================================================\n",
            "(shufflenet), [63], loss : 6.003, accuracy : 94.769%\n",
            "(shufflenet), [64], loss : 5.891, accuracy : 94.938%\n",
            "(shufflenet), [65], loss : 5.781, accuracy : 95.388%\n",
            "(shufflenet), [66], loss : 5.673, accuracy : 95.726%\n",
            "(shufflenet), [67], loss : 5.567, accuracy : 95.838%\n",
            "(shufflenet), [68], loss : 5.462, accuracy : 96.007%\n",
            "(shufflenet), [69], loss : 5.360, accuracy : 96.344%\n",
            "(shufflenet), [70], loss : 5.259, accuracy : 96.625%\n",
            "(shufflenet), [71], loss : 5.161, accuracy : 96.738%\n",
            "======================================================\n",
            "[71], TEST accuracy : 82.472%\n",
            "======================================================\n",
            "(shufflenet), [72], loss : 5.064, accuracy : 96.907%\n",
            "(shufflenet), [73], loss : 4.969, accuracy : 97.019%\n",
            "(shufflenet), [74], loss : 4.875, accuracy : 97.188%\n",
            "(shufflenet), [75], loss : 4.784, accuracy : 97.300%\n",
            "(shufflenet), [76], loss : 4.694, accuracy : 97.525%\n",
            "(shufflenet), [77], loss : 4.606, accuracy : 97.582%\n",
            "(shufflenet), [78], loss : 4.520, accuracy : 97.694%\n",
            "(shufflenet), [79], loss : 4.436, accuracy : 97.750%\n",
            "(shufflenet), [80], loss : 4.353, accuracy : 98.032%\n",
            "======================================================\n",
            "[80], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(shufflenet), [81], loss : 4.272, accuracy : 98.144%\n",
            "(shufflenet), [82], loss : 4.193, accuracy : 98.369%\n",
            "(shufflenet), [83], loss : 4.115, accuracy : 98.425%\n",
            "(shufflenet), [84], loss : 4.039, accuracy : 98.481%\n",
            "(shufflenet), [85], loss : 3.964, accuracy : 98.538%\n",
            "(shufflenet), [86], loss : 3.891, accuracy : 98.706%\n",
            "(shufflenet), [87], loss : 3.820, accuracy : 98.763%\n",
            "(shufflenet), [88], loss : 3.750, accuracy : 98.763%\n",
            "(shufflenet), [89], loss : 3.681, accuracy : 98.875%\n",
            "======================================================\n",
            "[89], TEST accuracy : 81.573%\n",
            "======================================================\n",
            "(shufflenet), [90], loss : 3.614, accuracy : 98.931%\n",
            "(shufflenet), [91], loss : 3.548, accuracy : 99.044%\n",
            "(shufflenet), [92], loss : 3.484, accuracy : 99.156%\n",
            "(shufflenet), [93], loss : 3.421, accuracy : 99.269%\n",
            "(shufflenet), [94], loss : 3.360, accuracy : 99.269%\n",
            "(shufflenet), [95], loss : 3.300, accuracy : 99.325%\n",
            "(shufflenet), [96], loss : 3.241, accuracy : 99.325%\n",
            "(shufflenet), [97], loss : 3.184, accuracy : 99.325%\n",
            "(shufflenet), [98], loss : 3.127, accuracy : 99.381%\n",
            "======================================================\n",
            "[98], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(shufflenet), [99], loss : 3.072, accuracy : 99.438%\n",
            "171835904\n",
            "171835904\n",
            "(mobilenet), [0], loss : 16.835, accuracy : 67.492%\n",
            "(mobilenet), [1], loss : 17.146, accuracy : 67.773%\n",
            "(mobilenet), [2], loss : 16.818, accuracy : 68.054%\n",
            "(mobilenet), [3], loss : 16.533, accuracy : 69.573%\n",
            "(mobilenet), [4], loss : 16.207, accuracy : 69.854%\n",
            "(mobilenet), [5], loss : 16.103, accuracy : 70.247%\n",
            "(mobilenet), [6], loss : 15.828, accuracy : 71.147%\n",
            "(mobilenet), [7], loss : 15.569, accuracy : 71.597%\n",
            "(mobilenet), [8], loss : 15.408, accuracy : 71.597%\n",
            "======================================================\n",
            "[8], TEST accuracy : 75.281%\n",
            "======================================================\n",
            "(mobilenet), [9], loss : 15.395, accuracy : 72.272%\n",
            "(mobilenet), [10], loss : 15.254, accuracy : 72.666%\n",
            "(mobilenet), [11], loss : 15.079, accuracy : 72.891%\n",
            "(mobilenet), [12], loss : 15.126, accuracy : 72.891%\n",
            "(mobilenet), [13], loss : 14.773, accuracy : 73.566%\n",
            "(mobilenet), [14], loss : 14.731, accuracy : 74.016%\n",
            "(mobilenet), [15], loss : 14.697, accuracy : 74.747%\n",
            "(mobilenet), [16], loss : 14.498, accuracy : 74.072%\n",
            "(mobilenet), [17], loss : 14.411, accuracy : 75.141%\n",
            "======================================================\n",
            "[17], TEST accuracy : 77.528%\n",
            "======================================================\n",
            "(mobilenet), [18], loss : 14.509, accuracy : 74.916%\n",
            "(mobilenet), [19], loss : 14.207, accuracy : 75.253%\n",
            "(mobilenet), [20], loss : 14.145, accuracy : 75.028%\n",
            "(mobilenet), [21], loss : 14.004, accuracy : 75.422%\n",
            "(mobilenet), [22], loss : 13.920, accuracy : 76.153%\n",
            "(mobilenet), [23], loss : 13.792, accuracy : 75.984%\n",
            "(mobilenet), [24], loss : 13.690, accuracy : 76.490%\n",
            "(mobilenet), [25], loss : 13.715, accuracy : 76.884%\n",
            "(mobilenet), [26], loss : 13.772, accuracy : 77.222%\n",
            "======================================================\n",
            "[26], TEST accuracy : 80.225%\n",
            "======================================================\n",
            "(mobilenet), [27], loss : 13.600, accuracy : 76.209%\n",
            "(mobilenet), [28], loss : 13.422, accuracy : 77.053%\n",
            "(mobilenet), [29], loss : 13.219, accuracy : 77.165%\n",
            "(mobilenet), [30], loss : 13.196, accuracy : 77.953%\n",
            "(mobilenet), [31], loss : 13.210, accuracy : 77.840%\n",
            "(mobilenet), [32], loss : 12.942, accuracy : 78.740%\n",
            "(mobilenet), [33], loss : 12.965, accuracy : 78.234%\n",
            "(mobilenet), [34], loss : 12.806, accuracy : 78.403%\n",
            "(mobilenet), [35], loss : 12.814, accuracy : 78.628%\n",
            "======================================================\n",
            "[35], TEST accuracy : 81.348%\n",
            "======================================================\n",
            "(mobilenet), [36], loss : 12.746, accuracy : 78.909%\n",
            "(mobilenet), [37], loss : 12.659, accuracy : 79.415%\n",
            "(mobilenet), [38], loss : 12.387, accuracy : 80.259%\n",
            "(mobilenet), [39], loss : 12.440, accuracy : 80.540%\n",
            "(mobilenet), [40], loss : 12.196, accuracy : 80.765%\n",
            "(mobilenet), [41], loss : 12.103, accuracy : 81.327%\n",
            "(mobilenet), [42], loss : 12.036, accuracy : 81.890%\n",
            "(mobilenet), [43], loss : 11.862, accuracy : 82.002%\n",
            "(mobilenet), [44], loss : 11.974, accuracy : 80.877%\n",
            "======================================================\n",
            "[44], TEST accuracy : 81.573%\n",
            "======================================================\n",
            "(mobilenet), [45], loss : 11.744, accuracy : 81.946%\n",
            "(mobilenet), [46], loss : 11.703, accuracy : 80.709%\n",
            "(mobilenet), [47], loss : 11.513, accuracy : 82.452%\n",
            "(mobilenet), [48], loss : 11.544, accuracy : 83.015%\n",
            "(mobilenet), [49], loss : 11.415, accuracy : 82.396%\n",
            "(mobilenet), [50], loss : 11.328, accuracy : 82.733%\n",
            "(mobilenet), [51], loss : 11.317, accuracy : 82.621%\n",
            "(mobilenet), [52], loss : 11.141, accuracy : 83.183%\n",
            "(mobilenet), [53], loss : 10.972, accuracy : 83.858%\n",
            "======================================================\n",
            "[53], TEST accuracy : 82.022%\n",
            "======================================================\n",
            "(mobilenet), [54], loss : 10.949, accuracy : 84.139%\n",
            "(mobilenet), [55], loss : 10.988, accuracy : 83.127%\n",
            "(mobilenet), [56], loss : 10.775, accuracy : 84.252%\n",
            "(mobilenet), [57], loss : 10.756, accuracy : 84.871%\n",
            "(mobilenet), [58], loss : 10.496, accuracy : 84.364%\n",
            "(mobilenet), [59], loss : 10.628, accuracy : 84.308%\n",
            "(mobilenet), [60], loss : 10.388, accuracy : 83.746%\n",
            "(mobilenet), [61], loss : 10.525, accuracy : 84.421%\n",
            "(mobilenet), [62], loss : 10.136, accuracy : 85.208%\n",
            "======================================================\n",
            "[62], TEST accuracy : 82.472%\n",
            "======================================================\n",
            "(mobilenet), [63], loss : 9.999, accuracy : 85.883%\n",
            "(mobilenet), [64], loss : 9.981, accuracy : 85.771%\n",
            "(mobilenet), [65], loss : 9.852, accuracy : 86.670%\n",
            "(mobilenet), [66], loss : 9.862, accuracy : 85.883%\n",
            "(mobilenet), [67], loss : 9.850, accuracy : 85.489%\n",
            "(mobilenet), [68], loss : 9.664, accuracy : 86.108%\n",
            "(mobilenet), [69], loss : 9.746, accuracy : 86.164%\n",
            "(mobilenet), [70], loss : 9.441, accuracy : 86.895%\n",
            "(mobilenet), [71], loss : 9.404, accuracy : 86.164%\n",
            "======================================================\n",
            "[71], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(mobilenet), [72], loss : 9.564, accuracy : 86.952%\n",
            "(mobilenet), [73], loss : 9.519, accuracy : 87.008%\n",
            "(mobilenet), [74], loss : 9.241, accuracy : 87.064%\n",
            "(mobilenet), [75], loss : 9.404, accuracy : 86.670%\n",
            "(mobilenet), [76], loss : 9.109, accuracy : 87.345%\n",
            "(mobilenet), [77], loss : 9.163, accuracy : 87.008%\n",
            "(mobilenet), [78], loss : 8.924, accuracy : 88.076%\n",
            "(mobilenet), [79], loss : 8.979, accuracy : 88.133%\n",
            "(mobilenet), [80], loss : 8.726, accuracy : 87.964%\n",
            "======================================================\n",
            "[80], TEST accuracy : 82.247%\n",
            "======================================================\n",
            "(mobilenet), [81], loss : 8.785, accuracy : 88.526%\n",
            "(mobilenet), [82], loss : 8.583, accuracy : 88.920%\n",
            "(mobilenet), [83], loss : 8.414, accuracy : 88.301%\n",
            "(mobilenet), [84], loss : 8.715, accuracy : 87.627%\n",
            "(mobilenet), [85], loss : 8.549, accuracy : 89.258%\n",
            "(mobilenet), [86], loss : 8.499, accuracy : 88.245%\n",
            "(mobilenet), [87], loss : 8.409, accuracy : 89.258%\n",
            "(mobilenet), [88], loss : 8.236, accuracy : 88.864%\n",
            "(mobilenet), [89], loss : 8.121, accuracy : 89.933%\n",
            "======================================================\n",
            "[89], TEST accuracy : 81.348%\n",
            "======================================================\n",
            "(mobilenet), [90], loss : 8.079, accuracy : 89.595%\n",
            "(mobilenet), [91], loss : 8.068, accuracy : 89.033%\n",
            "(mobilenet), [92], loss : 7.931, accuracy : 89.651%\n",
            "(mobilenet), [93], loss : 7.965, accuracy : 90.270%\n",
            "(mobilenet), [94], loss : 7.739, accuracy : 90.326%\n",
            "(mobilenet), [95], loss : 7.723, accuracy : 90.720%\n",
            "(mobilenet), [96], loss : 7.497, accuracy : 90.776%\n",
            "(mobilenet), [97], loss : 7.571, accuracy : 91.001%\n",
            "(mobilenet), [98], loss : 7.451, accuracy : 90.832%\n",
            "======================================================\n",
            "[98], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(mobilenet), [99], loss : 7.600, accuracy : 89.933%\n",
            "186097152\n",
            "186097152\n",
            "(resnext50_32x4d), [0], loss : 16.993, accuracy : 66.817%\n",
            "(resnext50_32x4d), [1], loss : 17.723, accuracy : 65.748%\n",
            "(resnext50_32x4d), [2], loss : 17.297, accuracy : 67.379%\n",
            "(resnext50_32x4d), [3], loss : 17.059, accuracy : 68.673%\n",
            "(resnext50_32x4d), [4], loss : 16.880, accuracy : 69.629%\n",
            "(resnext50_32x4d), [5], loss : 16.725, accuracy : 70.360%\n",
            "(resnext50_32x4d), [6], loss : 16.583, accuracy : 71.260%\n",
            "(resnext50_32x4d), [7], loss : 16.451, accuracy : 71.260%\n",
            "(resnext50_32x4d), [8], loss : 16.325, accuracy : 71.654%\n",
            "======================================================\n",
            "[8], TEST accuracy : 71.685%\n",
            "======================================================\n",
            "(resnext50_32x4d), [9], loss : 16.205, accuracy : 71.991%\n",
            "(resnext50_32x4d), [10], loss : 16.091, accuracy : 72.047%\n",
            "(resnext50_32x4d), [11], loss : 15.980, accuracy : 72.553%\n",
            "(resnext50_32x4d), [12], loss : 15.873, accuracy : 72.722%\n",
            "(resnext50_32x4d), [13], loss : 15.768, accuracy : 73.172%\n",
            "(resnext50_32x4d), [14], loss : 15.668, accuracy : 73.566%\n",
            "(resnext50_32x4d), [15], loss : 15.569, accuracy : 74.016%\n",
            "(resnext50_32x4d), [16], loss : 15.472, accuracy : 74.128%\n",
            "(resnext50_32x4d), [17], loss : 15.377, accuracy : 74.353%\n",
            "======================================================\n",
            "[17], TEST accuracy : 74.157%\n",
            "======================================================\n",
            "(resnext50_32x4d), [18], loss : 15.283, accuracy : 74.578%\n",
            "(resnext50_32x4d), [19], loss : 15.191, accuracy : 74.803%\n",
            "(resnext50_32x4d), [20], loss : 15.100, accuracy : 75.253%\n",
            "(resnext50_32x4d), [21], loss : 15.010, accuracy : 75.478%\n",
            "(resnext50_32x4d), [22], loss : 14.920, accuracy : 75.872%\n",
            "(resnext50_32x4d), [23], loss : 14.831, accuracy : 75.984%\n",
            "(resnext50_32x4d), [24], loss : 14.744, accuracy : 76.153%\n",
            "(resnext50_32x4d), [25], loss : 14.657, accuracy : 76.209%\n",
            "(resnext50_32x4d), [26], loss : 14.571, accuracy : 76.547%\n",
            "======================================================\n",
            "[26], TEST accuracy : 75.506%\n",
            "======================================================\n",
            "(resnext50_32x4d), [27], loss : 14.485, accuracy : 76.772%\n",
            "(resnext50_32x4d), [28], loss : 14.399, accuracy : 77.053%\n",
            "(resnext50_32x4d), [29], loss : 14.314, accuracy : 77.278%\n",
            "(resnext50_32x4d), [30], loss : 14.229, accuracy : 77.390%\n",
            "(resnext50_32x4d), [31], loss : 14.144, accuracy : 77.615%\n",
            "(resnext50_32x4d), [32], loss : 14.060, accuracy : 77.728%\n",
            "(resnext50_32x4d), [33], loss : 13.976, accuracy : 77.840%\n",
            "(resnext50_32x4d), [34], loss : 13.892, accuracy : 77.784%\n",
            "(resnext50_32x4d), [35], loss : 13.809, accuracy : 78.065%\n",
            "======================================================\n",
            "[35], TEST accuracy : 75.955%\n",
            "======================================================\n",
            "(resnext50_32x4d), [36], loss : 13.725, accuracy : 78.346%\n",
            "(resnext50_32x4d), [37], loss : 13.641, accuracy : 78.459%\n",
            "(resnext50_32x4d), [38], loss : 13.558, accuracy : 78.459%\n",
            "(resnext50_32x4d), [39], loss : 13.474, accuracy : 78.515%\n",
            "(resnext50_32x4d), [40], loss : 13.391, accuracy : 78.571%\n",
            "(resnext50_32x4d), [41], loss : 13.309, accuracy : 78.965%\n",
            "(resnext50_32x4d), [42], loss : 13.225, accuracy : 79.190%\n",
            "(resnext50_32x4d), [43], loss : 13.141, accuracy : 79.415%\n",
            "(resnext50_32x4d), [44], loss : 13.058, accuracy : 79.753%\n",
            "======================================================\n",
            "[44], TEST accuracy : 76.629%\n",
            "======================================================\n",
            "(resnext50_32x4d), [45], loss : 12.975, accuracy : 80.034%\n",
            "(resnext50_32x4d), [46], loss : 12.891, accuracy : 80.315%\n",
            "(resnext50_32x4d), [47], loss : 12.807, accuracy : 80.371%\n",
            "(resnext50_32x4d), [48], loss : 12.724, accuracy : 80.540%\n",
            "(resnext50_32x4d), [49], loss : 12.640, accuracy : 80.709%\n",
            "(resnext50_32x4d), [50], loss : 12.557, accuracy : 80.821%\n",
            "(resnext50_32x4d), [51], loss : 12.472, accuracy : 81.215%\n",
            "(resnext50_32x4d), [52], loss : 12.388, accuracy : 81.384%\n",
            "(resnext50_32x4d), [53], loss : 12.305, accuracy : 81.609%\n",
            "======================================================\n",
            "[53], TEST accuracy : 76.180%\n",
            "======================================================\n",
            "(resnext50_32x4d), [54], loss : 12.220, accuracy : 81.834%\n",
            "(resnext50_32x4d), [55], loss : 12.136, accuracy : 82.058%\n",
            "(resnext50_32x4d), [56], loss : 12.052, accuracy : 82.227%\n",
            "(resnext50_32x4d), [57], loss : 11.969, accuracy : 82.396%\n",
            "(resnext50_32x4d), [58], loss : 11.884, accuracy : 82.452%\n",
            "(resnext50_32x4d), [59], loss : 11.800, accuracy : 82.846%\n",
            "(resnext50_32x4d), [60], loss : 11.718, accuracy : 82.902%\n",
            "(resnext50_32x4d), [61], loss : 11.633, accuracy : 83.015%\n",
            "(resnext50_32x4d), [62], loss : 11.549, accuracy : 83.183%\n",
            "======================================================\n",
            "[62], TEST accuracy : 77.079%\n",
            "======================================================\n",
            "(resnext50_32x4d), [63], loss : 11.466, accuracy : 83.240%\n",
            "(resnext50_32x4d), [64], loss : 11.382, accuracy : 83.408%\n",
            "(resnext50_32x4d), [65], loss : 11.299, accuracy : 83.690%\n",
            "(resnext50_32x4d), [66], loss : 11.216, accuracy : 83.858%\n",
            "(resnext50_32x4d), [67], loss : 11.133, accuracy : 83.802%\n",
            "(resnext50_32x4d), [68], loss : 11.050, accuracy : 83.915%\n",
            "(resnext50_32x4d), [69], loss : 10.968, accuracy : 84.083%\n",
            "(resnext50_32x4d), [70], loss : 10.885, accuracy : 84.364%\n",
            "(resnext50_32x4d), [71], loss : 10.802, accuracy : 84.758%\n",
            "======================================================\n",
            "[71], TEST accuracy : 77.528%\n",
            "======================================================\n",
            "(resnext50_32x4d), [72], loss : 10.719, accuracy : 84.983%\n",
            "(resnext50_32x4d), [73], loss : 10.638, accuracy : 85.152%\n",
            "(resnext50_32x4d), [74], loss : 10.555, accuracy : 85.264%\n",
            "(resnext50_32x4d), [75], loss : 10.474, accuracy : 85.546%\n",
            "(resnext50_32x4d), [76], loss : 10.391, accuracy : 85.771%\n",
            "(resnext50_32x4d), [77], loss : 10.310, accuracy : 85.883%\n",
            "(resnext50_32x4d), [78], loss : 10.228, accuracy : 86.108%\n",
            "(resnext50_32x4d), [79], loss : 10.148, accuracy : 86.389%\n",
            "(resnext50_32x4d), [80], loss : 10.066, accuracy : 86.502%\n",
            "======================================================\n",
            "[80], TEST accuracy : 78.427%\n",
            "======================================================\n",
            "(resnext50_32x4d), [81], loss : 9.986, accuracy : 86.727%\n",
            "(resnext50_32x4d), [82], loss : 9.905, accuracy : 87.008%\n",
            "(resnext50_32x4d), [83], loss : 9.825, accuracy : 87.177%\n",
            "(resnext50_32x4d), [84], loss : 9.745, accuracy : 87.233%\n",
            "(resnext50_32x4d), [85], loss : 9.665, accuracy : 87.458%\n",
            "(resnext50_32x4d), [86], loss : 9.586, accuracy : 87.570%\n",
            "(resnext50_32x4d), [87], loss : 9.506, accuracy : 87.908%\n",
            "(resnext50_32x4d), [88], loss : 9.427, accuracy : 88.245%\n",
            "(resnext50_32x4d), [89], loss : 9.349, accuracy : 88.414%\n",
            "======================================================\n",
            "[89], TEST accuracy : 77.978%\n",
            "======================================================\n",
            "(resnext50_32x4d), [90], loss : 9.271, accuracy : 88.583%\n",
            "(resnext50_32x4d), [91], loss : 9.193, accuracy : 88.695%\n",
            "(resnext50_32x4d), [92], loss : 9.115, accuracy : 88.751%\n",
            "(resnext50_32x4d), [93], loss : 9.039, accuracy : 89.033%\n",
            "(resnext50_32x4d), [94], loss : 8.961, accuracy : 89.145%\n",
            "(resnext50_32x4d), [95], loss : 8.885, accuracy : 89.370%\n",
            "(resnext50_32x4d), [96], loss : 8.809, accuracy : 89.595%\n",
            "(resnext50_32x4d), [97], loss : 8.733, accuracy : 89.595%\n",
            "(resnext50_32x4d), [98], loss : 8.658, accuracy : 89.820%\n",
            "======================================================\n",
            "[98], TEST accuracy : 77.978%\n",
            "======================================================\n",
            "(resnext50_32x4d), [99], loss : 8.583, accuracy : 89.989%\n",
            "219362816\n",
            "219362816\n",
            "(wide_resnet50_2), [0], loss : 17.326, accuracy : 67.042%\n",
            "(wide_resnet50_2), [1], loss : 18.235, accuracy : 65.242%\n",
            "(wide_resnet50_2), [2], loss : 17.910, accuracy : 65.636%\n",
            "(wide_resnet50_2), [3], loss : 17.699, accuracy : 67.098%\n",
            "(wide_resnet50_2), [4], loss : 17.541, accuracy : 67.604%\n",
            "(wide_resnet50_2), [5], loss : 17.409, accuracy : 67.942%\n",
            "(wide_resnet50_2), [6], loss : 17.293, accuracy : 68.504%\n",
            "(wide_resnet50_2), [7], loss : 17.186, accuracy : 69.235%\n",
            "(wide_resnet50_2), [8], loss : 17.086, accuracy : 69.348%\n",
            "======================================================\n",
            "[8], TEST accuracy : 70.787%\n",
            "======================================================\n",
            "(wide_resnet50_2), [9], loss : 16.990, accuracy : 69.348%\n",
            "(wide_resnet50_2), [10], loss : 16.900, accuracy : 69.516%\n",
            "(wide_resnet50_2), [11], loss : 16.813, accuracy : 69.629%\n",
            "(wide_resnet50_2), [12], loss : 16.728, accuracy : 69.798%\n",
            "(wide_resnet50_2), [13], loss : 16.646, accuracy : 70.022%\n",
            "(wide_resnet50_2), [14], loss : 16.566, accuracy : 70.304%\n",
            "(wide_resnet50_2), [15], loss : 16.488, accuracy : 70.585%\n",
            "(wide_resnet50_2), [16], loss : 16.411, accuracy : 70.697%\n",
            "(wide_resnet50_2), [17], loss : 16.335, accuracy : 70.922%\n",
            "======================================================\n",
            "[17], TEST accuracy : 71.685%\n",
            "======================================================\n",
            "(wide_resnet50_2), [18], loss : 16.260, accuracy : 71.204%\n",
            "(wide_resnet50_2), [19], loss : 16.187, accuracy : 71.372%\n",
            "(wide_resnet50_2), [20], loss : 16.114, accuracy : 71.541%\n",
            "(wide_resnet50_2), [21], loss : 16.041, accuracy : 71.822%\n",
            "(wide_resnet50_2), [22], loss : 15.969, accuracy : 72.047%\n",
            "(wide_resnet50_2), [23], loss : 15.898, accuracy : 72.160%\n",
            "(wide_resnet50_2), [24], loss : 15.828, accuracy : 72.216%\n",
            "(wide_resnet50_2), [25], loss : 15.758, accuracy : 72.385%\n",
            "(wide_resnet50_2), [26], loss : 15.688, accuracy : 72.553%\n",
            "======================================================\n",
            "[26], TEST accuracy : 73.034%\n",
            "======================================================\n",
            "(wide_resnet50_2), [27], loss : 15.619, accuracy : 72.722%\n",
            "(wide_resnet50_2), [28], loss : 15.551, accuracy : 72.891%\n",
            "(wide_resnet50_2), [29], loss : 15.482, accuracy : 73.003%\n",
            "(wide_resnet50_2), [30], loss : 15.414, accuracy : 73.228%\n",
            "(wide_resnet50_2), [31], loss : 15.346, accuracy : 73.397%\n",
            "(wide_resnet50_2), [32], loss : 15.278, accuracy : 73.397%\n",
            "(wide_resnet50_2), [33], loss : 15.210, accuracy : 73.566%\n",
            "(wide_resnet50_2), [34], loss : 15.142, accuracy : 73.791%\n",
            "(wide_resnet50_2), [35], loss : 15.074, accuracy : 73.903%\n",
            "======================================================\n",
            "[35], TEST accuracy : 73.933%\n",
            "======================================================\n",
            "(wide_resnet50_2), [36], loss : 15.007, accuracy : 74.072%\n",
            "(wide_resnet50_2), [37], loss : 14.939, accuracy : 74.353%\n",
            "(wide_resnet50_2), [38], loss : 14.872, accuracy : 74.409%\n",
            "(wide_resnet50_2), [39], loss : 14.805, accuracy : 74.522%\n",
            "(wide_resnet50_2), [40], loss : 14.738, accuracy : 74.691%\n",
            "(wide_resnet50_2), [41], loss : 14.670, accuracy : 74.691%\n",
            "(wide_resnet50_2), [42], loss : 14.604, accuracy : 74.747%\n",
            "(wide_resnet50_2), [43], loss : 14.537, accuracy : 74.972%\n",
            "(wide_resnet50_2), [44], loss : 14.471, accuracy : 75.141%\n",
            "======================================================\n",
            "[44], TEST accuracy : 74.382%\n",
            "======================================================\n",
            "(wide_resnet50_2), [45], loss : 14.405, accuracy : 75.309%\n",
            "(wide_resnet50_2), [46], loss : 14.338, accuracy : 75.478%\n",
            "(wide_resnet50_2), [47], loss : 14.272, accuracy : 75.591%\n",
            "(wide_resnet50_2), [48], loss : 14.205, accuracy : 75.703%\n",
            "(wide_resnet50_2), [49], loss : 14.139, accuracy : 75.703%\n",
            "(wide_resnet50_2), [50], loss : 14.073, accuracy : 75.928%\n",
            "(wide_resnet50_2), [51], loss : 14.007, accuracy : 76.209%\n",
            "(wide_resnet50_2), [52], loss : 13.941, accuracy : 76.378%\n",
            "(wide_resnet50_2), [53], loss : 13.875, accuracy : 76.322%\n",
            "======================================================\n",
            "[53], TEST accuracy : 74.607%\n",
            "======================================================\n",
            "(wide_resnet50_2), [54], loss : 13.809, accuracy : 76.490%\n",
            "(wide_resnet50_2), [55], loss : 13.743, accuracy : 76.490%\n",
            "(wide_resnet50_2), [56], loss : 13.677, accuracy : 76.715%\n",
            "(wide_resnet50_2), [57], loss : 13.610, accuracy : 76.940%\n",
            "(wide_resnet50_2), [58], loss : 13.544, accuracy : 77.222%\n",
            "(wide_resnet50_2), [59], loss : 13.478, accuracy : 77.672%\n",
            "(wide_resnet50_2), [60], loss : 13.412, accuracy : 78.009%\n",
            "(wide_resnet50_2), [61], loss : 13.345, accuracy : 78.178%\n",
            "(wide_resnet50_2), [62], loss : 13.279, accuracy : 78.403%\n",
            "======================================================\n",
            "[62], TEST accuracy : 75.281%\n",
            "======================================================\n",
            "(wide_resnet50_2), [63], loss : 13.214, accuracy : 78.571%\n",
            "(wide_resnet50_2), [64], loss : 13.147, accuracy : 78.740%\n",
            "(wide_resnet50_2), [65], loss : 13.081, accuracy : 79.078%\n",
            "(wide_resnet50_2), [66], loss : 13.015, accuracy : 79.190%\n",
            "(wide_resnet50_2), [67], loss : 12.949, accuracy : 79.359%\n",
            "(wide_resnet50_2), [68], loss : 12.883, accuracy : 79.640%\n",
            "(wide_resnet50_2), [69], loss : 12.817, accuracy : 79.696%\n",
            "(wide_resnet50_2), [70], loss : 12.751, accuracy : 80.034%\n",
            "(wide_resnet50_2), [71], loss : 12.686, accuracy : 80.090%\n",
            "======================================================\n",
            "[71], TEST accuracy : 76.404%\n",
            "======================================================\n",
            "(wide_resnet50_2), [72], loss : 12.620, accuracy : 80.427%\n",
            "(wide_resnet50_2), [73], loss : 12.555, accuracy : 80.596%\n",
            "(wide_resnet50_2), [74], loss : 12.489, accuracy : 80.877%\n",
            "(wide_resnet50_2), [75], loss : 12.424, accuracy : 80.877%\n",
            "(wide_resnet50_2), [76], loss : 12.357, accuracy : 80.990%\n",
            "(wide_resnet50_2), [77], loss : 12.292, accuracy : 80.990%\n",
            "(wide_resnet50_2), [78], loss : 12.226, accuracy : 81.102%\n",
            "(wide_resnet50_2), [79], loss : 12.160, accuracy : 81.215%\n",
            "(wide_resnet50_2), [80], loss : 12.095, accuracy : 81.215%\n",
            "======================================================\n",
            "[80], TEST accuracy : 77.303%\n",
            "======================================================\n",
            "(wide_resnet50_2), [81], loss : 12.029, accuracy : 81.496%\n",
            "(wide_resnet50_2), [82], loss : 11.964, accuracy : 81.609%\n",
            "(wide_resnet50_2), [83], loss : 11.899, accuracy : 81.777%\n",
            "(wide_resnet50_2), [84], loss : 11.833, accuracy : 81.946%\n",
            "(wide_resnet50_2), [85], loss : 11.768, accuracy : 82.058%\n",
            "(wide_resnet50_2), [86], loss : 11.702, accuracy : 82.283%\n",
            "(wide_resnet50_2), [87], loss : 11.637, accuracy : 82.508%\n",
            "(wide_resnet50_2), [88], loss : 11.571, accuracy : 82.958%\n",
            "(wide_resnet50_2), [89], loss : 11.506, accuracy : 83.071%\n",
            "======================================================\n",
            "[89], TEST accuracy : 78.202%\n",
            "======================================================\n",
            "(wide_resnet50_2), [90], loss : 11.441, accuracy : 83.240%\n",
            "(wide_resnet50_2), [91], loss : 11.376, accuracy : 83.352%\n",
            "(wide_resnet50_2), [92], loss : 11.312, accuracy : 83.408%\n",
            "(wide_resnet50_2), [93], loss : 11.247, accuracy : 83.577%\n",
            "(wide_resnet50_2), [94], loss : 11.182, accuracy : 83.802%\n",
            "(wide_resnet50_2), [95], loss : 11.117, accuracy : 83.802%\n",
            "(wide_resnet50_2), [96], loss : 11.052, accuracy : 83.858%\n",
            "(wide_resnet50_2), [97], loss : 10.988, accuracy : 83.858%\n",
            "(wide_resnet50_2), [98], loss : 10.923, accuracy : 83.915%\n",
            "======================================================\n",
            "[98], TEST accuracy : 78.202%\n",
            "======================================================\n",
            "(wide_resnet50_2), [99], loss : 10.859, accuracy : 84.139%\n",
            "495590912\n",
            "495590912\n",
            "(mnasnet), [0], loss : 16.548, accuracy : 70.135%\n",
            "(mnasnet), [1], loss : 16.941, accuracy : 70.191%\n",
            "(mnasnet), [2], loss : 16.681, accuracy : 70.247%\n",
            "(mnasnet), [3], loss : 16.271, accuracy : 71.597%\n",
            "(mnasnet), [4], loss : 15.919, accuracy : 71.260%\n",
            "(mnasnet), [5], loss : 15.827, accuracy : 71.372%\n",
            "(mnasnet), [6], loss : 15.624, accuracy : 72.610%\n",
            "(mnasnet), [7], loss : 15.363, accuracy : 72.947%\n",
            "(mnasnet), [8], loss : 15.221, accuracy : 72.272%\n",
            "======================================================\n",
            "[8], TEST accuracy : 72.360%\n",
            "======================================================\n",
            "(mnasnet), [9], loss : 15.124, accuracy : 72.947%\n",
            "(mnasnet), [10], loss : 14.880, accuracy : 74.016%\n",
            "(mnasnet), [11], loss : 14.773, accuracy : 73.847%\n",
            "(mnasnet), [12], loss : 14.545, accuracy : 74.353%\n",
            "(mnasnet), [13], loss : 14.468, accuracy : 74.241%\n",
            "(mnasnet), [14], loss : 14.413, accuracy : 74.803%\n",
            "(mnasnet), [15], loss : 14.319, accuracy : 74.859%\n",
            "(mnasnet), [16], loss : 14.042, accuracy : 76.097%\n",
            "(mnasnet), [17], loss : 13.911, accuracy : 75.928%\n",
            "======================================================\n",
            "[17], TEST accuracy : 74.607%\n",
            "======================================================\n",
            "(mnasnet), [18], loss : 13.925, accuracy : 76.097%\n",
            "(mnasnet), [19], loss : 13.672, accuracy : 76.828%\n",
            "(mnasnet), [20], loss : 13.612, accuracy : 76.997%\n",
            "(mnasnet), [21], loss : 13.735, accuracy : 76.659%\n",
            "(mnasnet), [22], loss : 13.497, accuracy : 77.447%\n",
            "(mnasnet), [23], loss : 13.507, accuracy : 77.334%\n",
            "(mnasnet), [24], loss : 13.287, accuracy : 77.222%\n",
            "(mnasnet), [25], loss : 13.162, accuracy : 78.234%\n",
            "(mnasnet), [26], loss : 13.282, accuracy : 77.897%\n",
            "======================================================\n",
            "[26], TEST accuracy : 75.955%\n",
            "======================================================\n",
            "(mnasnet), [27], loss : 13.120, accuracy : 77.334%\n",
            "(mnasnet), [28], loss : 13.162, accuracy : 77.784%\n",
            "(mnasnet), [29], loss : 12.729, accuracy : 79.021%\n",
            "(mnasnet), [30], loss : 12.856, accuracy : 78.628%\n",
            "(mnasnet), [31], loss : 12.703, accuracy : 78.853%\n",
            "(mnasnet), [32], loss : 12.579, accuracy : 79.190%\n",
            "(mnasnet), [33], loss : 12.582, accuracy : 79.528%\n",
            "(mnasnet), [34], loss : 12.210, accuracy : 80.371%\n",
            "(mnasnet), [35], loss : 12.402, accuracy : 79.978%\n",
            "======================================================\n",
            "[35], TEST accuracy : 74.831%\n",
            "======================================================\n",
            "(mnasnet), [36], loss : 12.181, accuracy : 80.034%\n",
            "(mnasnet), [37], loss : 12.099, accuracy : 80.315%\n",
            "(mnasnet), [38], loss : 12.074, accuracy : 80.315%\n",
            "(mnasnet), [39], loss : 11.908, accuracy : 80.540%\n",
            "(mnasnet), [40], loss : 11.747, accuracy : 81.159%\n",
            "(mnasnet), [41], loss : 11.577, accuracy : 81.384%\n",
            "(mnasnet), [42], loss : 11.669, accuracy : 80.934%\n",
            "(mnasnet), [43], loss : 11.468, accuracy : 81.946%\n",
            "(mnasnet), [44], loss : 11.422, accuracy : 82.002%\n",
            "======================================================\n",
            "[44], TEST accuracy : 75.056%\n",
            "======================================================\n",
            "(mnasnet), [45], loss : 11.277, accuracy : 83.577%\n",
            "(mnasnet), [46], loss : 11.125, accuracy : 82.790%\n",
            "(mnasnet), [47], loss : 11.240, accuracy : 82.171%\n",
            "(mnasnet), [48], loss : 10.964, accuracy : 83.352%\n",
            "(mnasnet), [49], loss : 11.014, accuracy : 82.508%\n",
            "(mnasnet), [50], loss : 10.788, accuracy : 83.465%\n",
            "(mnasnet), [51], loss : 10.775, accuracy : 83.633%\n",
            "(mnasnet), [52], loss : 10.665, accuracy : 84.083%\n",
            "(mnasnet), [53], loss : 10.520, accuracy : 83.746%\n",
            "======================================================\n",
            "[53], TEST accuracy : 75.506%\n",
            "======================================================\n",
            "(mnasnet), [54], loss : 10.534, accuracy : 83.971%\n",
            "(mnasnet), [55], loss : 10.133, accuracy : 84.814%\n",
            "(mnasnet), [56], loss : 10.375, accuracy : 83.633%\n",
            "(mnasnet), [57], loss : 10.077, accuracy : 84.364%\n",
            "(mnasnet), [58], loss : 10.304, accuracy : 84.027%\n",
            "(mnasnet), [59], loss : 10.198, accuracy : 84.421%\n",
            "(mnasnet), [60], loss : 9.842, accuracy : 86.108%\n",
            "(mnasnet), [61], loss : 9.724, accuracy : 85.658%\n",
            "(mnasnet), [62], loss : 9.763, accuracy : 85.714%\n",
            "======================================================\n",
            "[62], TEST accuracy : 76.629%\n",
            "======================================================\n",
            "(mnasnet), [63], loss : 9.618, accuracy : 86.445%\n",
            "(mnasnet), [64], loss : 9.477, accuracy : 85.883%\n",
            "(mnasnet), [65], loss : 9.655, accuracy : 85.714%\n",
            "(mnasnet), [66], loss : 9.174, accuracy : 86.952%\n",
            "(mnasnet), [67], loss : 9.228, accuracy : 86.164%\n",
            "(mnasnet), [68], loss : 9.196, accuracy : 86.783%\n",
            "(mnasnet), [69], loss : 9.139, accuracy : 87.289%\n",
            "(mnasnet), [70], loss : 9.135, accuracy : 86.333%\n",
            "(mnasnet), [71], loss : 8.786, accuracy : 88.020%\n",
            "======================================================\n",
            "[71], TEST accuracy : 76.854%\n",
            "======================================================\n",
            "(mnasnet), [72], loss : 8.899, accuracy : 87.177%\n",
            "(mnasnet), [73], loss : 8.733, accuracy : 88.189%\n",
            "(mnasnet), [74], loss : 8.869, accuracy : 87.514%\n",
            "(mnasnet), [75], loss : 8.708, accuracy : 88.133%\n",
            "(mnasnet), [76], loss : 8.480, accuracy : 87.908%\n",
            "(mnasnet), [77], loss : 8.268, accuracy : 89.089%\n",
            "(mnasnet), [78], loss : 8.438, accuracy : 88.020%\n",
            "(mnasnet), [79], loss : 8.285, accuracy : 88.245%\n",
            "(mnasnet), [80], loss : 8.322, accuracy : 88.189%\n",
            "======================================================\n",
            "[80], TEST accuracy : 75.730%\n",
            "======================================================\n",
            "(mnasnet), [81], loss : 8.212, accuracy : 88.414%\n",
            "(mnasnet), [82], loss : 8.086, accuracy : 89.089%\n",
            "(mnasnet), [83], loss : 8.012, accuracy : 89.370%\n",
            "(mnasnet), [84], loss : 7.767, accuracy : 89.989%\n",
            "(mnasnet), [85], loss : 7.877, accuracy : 89.651%\n",
            "(mnasnet), [86], loss : 7.742, accuracy : 90.101%\n",
            "(mnasnet), [87], loss : 7.659, accuracy : 90.214%\n",
            "(mnasnet), [88], loss : 7.776, accuracy : 89.651%\n",
            "(mnasnet), [89], loss : 7.601, accuracy : 90.495%\n",
            "======================================================\n",
            "[89], TEST accuracy : 76.404%\n",
            "======================================================\n",
            "(mnasnet), [90], loss : 7.662, accuracy : 90.439%\n",
            "(mnasnet), [91], loss : 7.450, accuracy : 90.664%\n",
            "(mnasnet), [92], loss : 7.165, accuracy : 90.776%\n",
            "(mnasnet), [93], loss : 7.356, accuracy : 90.214%\n",
            "(mnasnet), [94], loss : 7.279, accuracy : 90.551%\n",
            "(mnasnet), [95], loss : 7.252, accuracy : 91.001%\n",
            "(mnasnet), [96], loss : 7.056, accuracy : 91.564%\n",
            "(mnasnet), [97], loss : 7.061, accuracy : 91.114%\n",
            "(mnasnet), [98], loss : 6.967, accuracy : 90.776%\n",
            "======================================================\n",
            "[98], TEST accuracy : 75.281%\n",
            "======================================================\n",
            "(mnasnet), [99], loss : 6.913, accuracy : 91.339%\n",
            "582575616\n",
            "582575616\n",
            "(resnet18), [0], loss : 16.602, accuracy : 67.998%\n",
            "(resnet18), [1], loss : 17.264, accuracy : 65.579%\n",
            "(resnet18), [2], loss : 16.714, accuracy : 67.435%\n",
            "(resnet18), [3], loss : 16.330, accuracy : 69.291%\n",
            "(resnet18), [4], loss : 16.022, accuracy : 70.247%\n",
            "(resnet18), [5], loss : 15.761, accuracy : 71.485%\n",
            "(resnet18), [6], loss : 15.531, accuracy : 72.497%\n",
            "(resnet18), [7], loss : 15.324, accuracy : 73.397%\n",
            "(resnet18), [8], loss : 15.133, accuracy : 73.791%\n",
            "======================================================\n",
            "[8], TEST accuracy : 72.809%\n",
            "======================================================\n",
            "(resnet18), [9], loss : 14.956, accuracy : 74.522%\n",
            "(resnet18), [10], loss : 14.789, accuracy : 74.747%\n",
            "(resnet18), [11], loss : 14.631, accuracy : 74.691%\n",
            "(resnet18), [12], loss : 14.480, accuracy : 74.803%\n",
            "(resnet18), [13], loss : 14.334, accuracy : 74.859%\n",
            "(resnet18), [14], loss : 14.191, accuracy : 75.309%\n",
            "(resnet18), [15], loss : 14.054, accuracy : 75.759%\n",
            "(resnet18), [16], loss : 13.918, accuracy : 75.984%\n",
            "(resnet18), [17], loss : 13.785, accuracy : 76.490%\n",
            "======================================================\n",
            "[17], TEST accuracy : 75.056%\n",
            "======================================================\n",
            "(resnet18), [18], loss : 13.654, accuracy : 76.715%\n",
            "(resnet18), [19], loss : 13.526, accuracy : 77.109%\n",
            "(resnet18), [20], loss : 13.400, accuracy : 77.278%\n",
            "(resnet18), [21], loss : 13.276, accuracy : 77.728%\n",
            "(resnet18), [22], loss : 13.153, accuracy : 78.121%\n",
            "(resnet18), [23], loss : 13.032, accuracy : 78.403%\n",
            "(resnet18), [24], loss : 12.913, accuracy : 78.628%\n",
            "(resnet18), [25], loss : 12.794, accuracy : 79.021%\n",
            "(resnet18), [26], loss : 12.677, accuracy : 79.303%\n",
            "======================================================\n",
            "[26], TEST accuracy : 77.303%\n",
            "======================================================\n",
            "(resnet18), [27], loss : 12.560, accuracy : 79.528%\n",
            "(resnet18), [28], loss : 12.444, accuracy : 79.921%\n",
            "(resnet18), [29], loss : 12.330, accuracy : 80.146%\n",
            "(resnet18), [30], loss : 12.217, accuracy : 80.484%\n",
            "(resnet18), [31], loss : 12.104, accuracy : 80.652%\n",
            "(resnet18), [32], loss : 11.992, accuracy : 80.877%\n",
            "(resnet18), [33], loss : 11.881, accuracy : 81.102%\n",
            "(resnet18), [34], loss : 11.771, accuracy : 81.271%\n",
            "(resnet18), [35], loss : 11.662, accuracy : 81.552%\n",
            "======================================================\n",
            "[35], TEST accuracy : 77.528%\n",
            "======================================================\n",
            "(resnet18), [36], loss : 11.553, accuracy : 81.946%\n",
            "(resnet18), [37], loss : 11.445, accuracy : 82.227%\n",
            "(resnet18), [38], loss : 11.338, accuracy : 82.452%\n",
            "(resnet18), [39], loss : 11.231, accuracy : 82.565%\n",
            "(resnet18), [40], loss : 11.125, accuracy : 82.902%\n",
            "(resnet18), [41], loss : 11.019, accuracy : 83.071%\n",
            "(resnet18), [42], loss : 10.915, accuracy : 83.521%\n",
            "(resnet18), [43], loss : 10.810, accuracy : 83.802%\n",
            "(resnet18), [44], loss : 10.707, accuracy : 83.971%\n",
            "======================================================\n",
            "[44], TEST accuracy : 78.427%\n",
            "======================================================\n",
            "(resnet18), [45], loss : 10.604, accuracy : 84.139%\n",
            "(resnet18), [46], loss : 10.502, accuracy : 84.139%\n",
            "(resnet18), [47], loss : 10.402, accuracy : 84.646%\n",
            "(resnet18), [48], loss : 10.301, accuracy : 84.814%\n",
            "(resnet18), [49], loss : 10.202, accuracy : 84.927%\n",
            "(resnet18), [50], loss : 10.104, accuracy : 85.039%\n",
            "(resnet18), [51], loss : 10.006, accuracy : 85.377%\n",
            "(resnet18), [52], loss : 9.909, accuracy : 85.827%\n",
            "(resnet18), [53], loss : 9.812, accuracy : 86.052%\n",
            "======================================================\n",
            "[53], TEST accuracy : 79.551%\n",
            "======================================================\n",
            "(resnet18), [54], loss : 9.716, accuracy : 86.445%\n",
            "(resnet18), [55], loss : 9.621, accuracy : 86.783%\n",
            "(resnet18), [56], loss : 9.527, accuracy : 86.895%\n",
            "(resnet18), [57], loss : 9.433, accuracy : 87.008%\n",
            "(resnet18), [58], loss : 9.340, accuracy : 87.177%\n",
            "(resnet18), [59], loss : 9.248, accuracy : 87.402%\n",
            "(resnet18), [60], loss : 9.156, accuracy : 87.739%\n",
            "(resnet18), [61], loss : 9.066, accuracy : 87.964%\n",
            "(resnet18), [62], loss : 8.977, accuracy : 88.133%\n",
            "======================================================\n",
            "[62], TEST accuracy : 80.225%\n",
            "======================================================\n",
            "(resnet18), [63], loss : 8.888, accuracy : 88.245%\n",
            "(resnet18), [64], loss : 8.800, accuracy : 88.639%\n",
            "(resnet18), [65], loss : 8.713, accuracy : 88.808%\n",
            "(resnet18), [66], loss : 8.627, accuracy : 88.976%\n",
            "(resnet18), [67], loss : 8.541, accuracy : 89.033%\n",
            "(resnet18), [68], loss : 8.456, accuracy : 89.258%\n",
            "(resnet18), [69], loss : 8.371, accuracy : 89.314%\n",
            "(resnet18), [70], loss : 8.288, accuracy : 89.708%\n",
            "(resnet18), [71], loss : 8.206, accuracy : 89.876%\n",
            "======================================================\n",
            "[71], TEST accuracy : 79.326%\n",
            "======================================================\n",
            "(resnet18), [72], loss : 8.123, accuracy : 89.933%\n",
            "(resnet18), [73], loss : 8.042, accuracy : 90.101%\n",
            "(resnet18), [74], loss : 7.961, accuracy : 90.326%\n",
            "(resnet18), [75], loss : 7.880, accuracy : 90.551%\n",
            "(resnet18), [76], loss : 7.801, accuracy : 90.551%\n",
            "(resnet18), [77], loss : 7.721, accuracy : 90.607%\n",
            "(resnet18), [78], loss : 7.643, accuracy : 90.832%\n",
            "(resnet18), [79], loss : 7.565, accuracy : 91.170%\n",
            "(resnet18), [80], loss : 7.489, accuracy : 91.114%\n",
            "======================================================\n",
            "[80], TEST accuracy : 79.551%\n",
            "======================================================\n",
            "(resnet18), [81], loss : 7.413, accuracy : 91.226%\n",
            "(resnet18), [82], loss : 7.337, accuracy : 91.395%\n",
            "(resnet18), [83], loss : 7.262, accuracy : 91.676%\n",
            "(resnet18), [84], loss : 7.188, accuracy : 91.789%\n",
            "(resnet18), [85], loss : 7.114, accuracy : 91.901%\n",
            "(resnet18), [86], loss : 7.040, accuracy : 92.182%\n",
            "(resnet18), [87], loss : 6.967, accuracy : 92.463%\n",
            "(resnet18), [88], loss : 6.896, accuracy : 92.520%\n",
            "(resnet18), [89], loss : 6.824, accuracy : 92.632%\n",
            "======================================================\n",
            "[89], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(resnet18), [90], loss : 6.753, accuracy : 92.632%\n",
            "(resnet18), [91], loss : 6.683, accuracy : 92.688%\n",
            "(resnet18), [92], loss : 6.613, accuracy : 92.857%\n",
            "(resnet18), [93], loss : 6.545, accuracy : 92.913%\n",
            "(resnet18), [94], loss : 6.477, accuracy : 93.082%\n",
            "(resnet18), [95], loss : 6.410, accuracy : 93.195%\n",
            "(resnet18), [96], loss : 6.342, accuracy : 93.195%\n",
            "(resnet18), [97], loss : 6.275, accuracy : 93.363%\n",
            "(resnet18), [98], loss : 6.209, accuracy : 93.476%\n",
            "======================================================\n",
            "[98], TEST accuracy : 80.449%\n",
            "======================================================\n",
            "(resnet18), [99], loss : 6.145, accuracy : 93.645%\n",
            "629552128\n",
            "629552128\n",
            "(alexnet), [0], loss : 16.032, accuracy : 70.472%\n",
            "(alexnet), [1], loss : 16.144, accuracy : 73.453%\n",
            "(alexnet), [2], loss : 15.536, accuracy : 72.722%\n",
            "(alexnet), [3], loss : 15.599, accuracy : 73.228%\n",
            "(alexnet), [4], loss : 15.185, accuracy : 73.453%\n",
            "(alexnet), [5], loss : 14.912, accuracy : 74.691%\n",
            "(alexnet), [6], loss : 14.752, accuracy : 74.409%\n",
            "(alexnet), [7], loss : 14.559, accuracy : 74.578%\n",
            "(alexnet), [8], loss : 14.566, accuracy : 74.916%\n",
            "======================================================\n",
            "[8], TEST accuracy : 71.910%\n",
            "======================================================\n",
            "(alexnet), [9], loss : 14.664, accuracy : 74.184%\n",
            "(alexnet), [10], loss : 14.559, accuracy : 74.634%\n",
            "(alexnet), [11], loss : 14.306, accuracy : 74.972%\n",
            "(alexnet), [12], loss : 14.204, accuracy : 75.028%\n",
            "(alexnet), [13], loss : 14.309, accuracy : 74.241%\n",
            "(alexnet), [14], loss : 14.209, accuracy : 74.072%\n",
            "(alexnet), [15], loss : 14.120, accuracy : 74.634%\n",
            "(alexnet), [16], loss : 14.052, accuracy : 74.747%\n",
            "(alexnet), [17], loss : 14.097, accuracy : 74.241%\n",
            "======================================================\n",
            "[17], TEST accuracy : 72.584%\n",
            "======================================================\n",
            "(alexnet), [18], loss : 13.899, accuracy : 74.522%\n",
            "(alexnet), [19], loss : 13.968, accuracy : 75.534%\n",
            "(alexnet), [20], loss : 13.873, accuracy : 74.803%\n",
            "(alexnet), [21], loss : 13.807, accuracy : 74.803%\n",
            "(alexnet), [22], loss : 13.912, accuracy : 74.522%\n",
            "(alexnet), [23], loss : 13.967, accuracy : 74.241%\n",
            "(alexnet), [24], loss : 13.887, accuracy : 73.903%\n",
            "(alexnet), [25], loss : 14.024, accuracy : 74.184%\n",
            "(alexnet), [26], loss : 13.936, accuracy : 74.241%\n",
            "======================================================\n",
            "[26], TEST accuracy : 73.708%\n",
            "======================================================\n",
            "(alexnet), [27], loss : 13.863, accuracy : 74.466%\n",
            "(alexnet), [28], loss : 13.787, accuracy : 74.859%\n",
            "(alexnet), [29], loss : 13.748, accuracy : 74.522%\n",
            "(alexnet), [30], loss : 13.800, accuracy : 74.184%\n",
            "(alexnet), [31], loss : 13.679, accuracy : 74.634%\n",
            "(alexnet), [32], loss : 13.527, accuracy : 75.141%\n",
            "(alexnet), [33], loss : 13.527, accuracy : 75.703%\n",
            "(alexnet), [34], loss : 13.678, accuracy : 74.578%\n",
            "(alexnet), [35], loss : 13.533, accuracy : 75.197%\n",
            "======================================================\n",
            "[35], TEST accuracy : 74.831%\n",
            "======================================================\n",
            "(alexnet), [36], loss : 13.528, accuracy : 74.409%\n",
            "(alexnet), [37], loss : 13.565, accuracy : 75.253%\n",
            "(alexnet), [38], loss : 13.572, accuracy : 74.353%\n",
            "(alexnet), [39], loss : 13.769, accuracy : 75.309%\n",
            "(alexnet), [40], loss : 13.695, accuracy : 74.072%\n",
            "(alexnet), [41], loss : 13.498, accuracy : 75.366%\n",
            "(alexnet), [42], loss : 13.388, accuracy : 74.972%\n",
            "(alexnet), [43], loss : 13.584, accuracy : 74.916%\n",
            "(alexnet), [44], loss : 13.492, accuracy : 75.028%\n",
            "======================================================\n",
            "[44], TEST accuracy : 74.382%\n",
            "======================================================\n",
            "(alexnet), [45], loss : 13.450, accuracy : 75.028%\n",
            "(alexnet), [46], loss : 13.629, accuracy : 74.128%\n",
            "(alexnet), [47], loss : 13.342, accuracy : 75.422%\n",
            "(alexnet), [48], loss : 13.344, accuracy : 74.747%\n",
            "(alexnet), [49], loss : 13.282, accuracy : 76.322%\n",
            "(alexnet), [50], loss : 13.354, accuracy : 75.028%\n",
            "(alexnet), [51], loss : 13.102, accuracy : 76.884%\n",
            "(alexnet), [52], loss : 13.014, accuracy : 76.772%\n",
            "(alexnet), [53], loss : 13.195, accuracy : 75.872%\n",
            "======================================================\n",
            "[53], TEST accuracy : 75.730%\n",
            "======================================================\n",
            "(alexnet), [54], loss : 13.141, accuracy : 75.759%\n",
            "(alexnet), [55], loss : 13.133, accuracy : 76.040%\n",
            "(alexnet), [56], loss : 13.253, accuracy : 75.309%\n",
            "(alexnet), [57], loss : 13.132, accuracy : 75.422%\n",
            "(alexnet), [58], loss : 13.264, accuracy : 75.591%\n",
            "(alexnet), [59], loss : 13.181, accuracy : 75.984%\n",
            "(alexnet), [60], loss : 13.073, accuracy : 76.603%\n",
            "(alexnet), [61], loss : 13.142, accuracy : 76.884%\n",
            "(alexnet), [62], loss : 13.007, accuracy : 76.715%\n",
            "======================================================\n",
            "[62], TEST accuracy : 76.854%\n",
            "======================================================\n",
            "(alexnet), [63], loss : 13.157, accuracy : 76.040%\n",
            "(alexnet), [64], loss : 13.213, accuracy : 76.153%\n",
            "(alexnet), [65], loss : 13.286, accuracy : 75.534%\n",
            "(alexnet), [66], loss : 13.190, accuracy : 75.984%\n",
            "(alexnet), [67], loss : 13.171, accuracy : 75.984%\n",
            "(alexnet), [68], loss : 12.883, accuracy : 76.828%\n",
            "(alexnet), [69], loss : 13.215, accuracy : 75.759%\n",
            "(alexnet), [70], loss : 13.005, accuracy : 76.490%\n",
            "(alexnet), [71], loss : 13.297, accuracy : 75.759%\n",
            "======================================================\n",
            "[71], TEST accuracy : 76.854%\n",
            "======================================================\n",
            "(alexnet), [72], loss : 13.107, accuracy : 76.322%\n",
            "(alexnet), [73], loss : 13.003, accuracy : 77.390%\n",
            "(alexnet), [74], loss : 13.181, accuracy : 76.434%\n",
            "(alexnet), [75], loss : 12.980, accuracy : 77.278%\n",
            "(alexnet), [76], loss : 13.103, accuracy : 76.490%\n",
            "(alexnet), [77], loss : 12.778, accuracy : 76.884%\n",
            "(alexnet), [78], loss : 12.924, accuracy : 77.222%\n",
            "(alexnet), [79], loss : 12.883, accuracy : 77.334%\n",
            "(alexnet), [80], loss : 12.894, accuracy : 76.659%\n",
            "======================================================\n",
            "[80], TEST accuracy : 77.079%\n",
            "======================================================\n",
            "(alexnet), [81], loss : 12.913, accuracy : 76.715%\n",
            "(alexnet), [82], loss : 12.802, accuracy : 77.109%\n",
            "(alexnet), [83], loss : 12.689, accuracy : 77.109%\n",
            "(alexnet), [84], loss : 13.039, accuracy : 76.884%\n",
            "(alexnet), [85], loss : 12.979, accuracy : 77.840%\n",
            "(alexnet), [86], loss : 12.689, accuracy : 76.940%\n",
            "(alexnet), [87], loss : 12.661, accuracy : 77.559%\n",
            "(alexnet), [88], loss : 12.985, accuracy : 77.165%\n",
            "(alexnet), [89], loss : 12.697, accuracy : 76.772%\n",
            "======================================================\n",
            "[89], TEST accuracy : 78.652%\n",
            "======================================================\n",
            "(alexnet), [90], loss : 12.554, accuracy : 77.503%\n",
            "(alexnet), [91], loss : 12.751, accuracy : 78.065%\n",
            "(alexnet), [92], loss : 12.608, accuracy : 77.953%\n",
            "(alexnet), [93], loss : 12.590, accuracy : 77.053%\n",
            "(alexnet), [94], loss : 12.822, accuracy : 77.953%\n",
            "(alexnet), [95], loss : 12.602, accuracy : 78.121%\n",
            "(alexnet), [96], loss : 12.523, accuracy : 78.065%\n",
            "(alexnet), [97], loss : 12.614, accuracy : 77.728%\n",
            "(alexnet), [98], loss : 12.641, accuracy : 77.840%\n",
            "======================================================\n",
            "[98], TEST accuracy : 77.079%\n",
            "======================================================\n",
            "(alexnet), [99], loss : 12.575, accuracy : 78.290%\n",
            "874835456\n",
            "874835456\n",
            "(squeezenet), [0], loss : 24.907, accuracy : 67.604%\n",
            "(squeezenet), [1], loss : 20.044, accuracy : 69.685%\n",
            "(squeezenet), [2], loss : 17.358, accuracy : 69.798%\n",
            "(squeezenet), [3], loss : 16.194, accuracy : 70.529%\n",
            "(squeezenet), [4], loss : 15.495, accuracy : 71.766%\n",
            "(squeezenet), [5], loss : 14.908, accuracy : 71.935%\n",
            "(squeezenet), [6], loss : 14.547, accuracy : 71.822%\n",
            "(squeezenet), [7], loss : 14.321, accuracy : 72.553%\n",
            "(squeezenet), [8], loss : 14.055, accuracy : 72.497%\n",
            "======================================================\n",
            "[8], TEST accuracy : 75.056%\n",
            "======================================================\n",
            "(squeezenet), [9], loss : 13.830, accuracy : 73.116%\n",
            "(squeezenet), [10], loss : 13.704, accuracy : 73.172%\n",
            "(squeezenet), [11], loss : 13.526, accuracy : 74.072%\n",
            "(squeezenet), [12], loss : 13.470, accuracy : 74.466%\n",
            "(squeezenet), [13], loss : 13.278, accuracy : 74.297%\n",
            "(squeezenet), [14], loss : 13.219, accuracy : 74.578%\n",
            "(squeezenet), [15], loss : 13.158, accuracy : 74.916%\n",
            "(squeezenet), [16], loss : 13.044, accuracy : 75.422%\n",
            "(squeezenet), [17], loss : 12.966, accuracy : 75.759%\n",
            "======================================================\n",
            "[17], TEST accuracy : 77.079%\n",
            "======================================================\n",
            "(squeezenet), [18], loss : 12.914, accuracy : 75.872%\n",
            "(squeezenet), [19], loss : 12.803, accuracy : 76.040%\n",
            "(squeezenet), [20], loss : 12.810, accuracy : 75.984%\n",
            "(squeezenet), [21], loss : 12.718, accuracy : 76.378%\n",
            "(squeezenet), [22], loss : 12.615, accuracy : 76.940%\n",
            "(squeezenet), [23], loss : 12.597, accuracy : 76.547%\n",
            "(squeezenet), [24], loss : 12.579, accuracy : 76.603%\n",
            "(squeezenet), [25], loss : 12.539, accuracy : 76.603%\n",
            "(squeezenet), [26], loss : 12.427, accuracy : 77.109%\n",
            "======================================================\n",
            "[26], TEST accuracy : 79.101%\n",
            "======================================================\n",
            "(squeezenet), [27], loss : 12.359, accuracy : 77.222%\n",
            "(squeezenet), [28], loss : 12.370, accuracy : 76.547%\n",
            "(squeezenet), [29], loss : 12.279, accuracy : 77.278%\n",
            "(squeezenet), [30], loss : 12.308, accuracy : 77.390%\n",
            "(squeezenet), [31], loss : 12.194, accuracy : 77.053%\n",
            "(squeezenet), [32], loss : 12.161, accuracy : 77.222%\n",
            "(squeezenet), [33], loss : 12.229, accuracy : 77.953%\n",
            "(squeezenet), [34], loss : 12.137, accuracy : 77.953%\n",
            "(squeezenet), [35], loss : 12.080, accuracy : 77.840%\n",
            "======================================================\n",
            "[35], TEST accuracy : 79.775%\n",
            "======================================================\n",
            "(squeezenet), [36], loss : 12.117, accuracy : 78.121%\n",
            "(squeezenet), [37], loss : 12.077, accuracy : 77.840%\n",
            "(squeezenet), [38], loss : 12.047, accuracy : 77.728%\n",
            "(squeezenet), [39], loss : 12.029, accuracy : 78.121%\n",
            "(squeezenet), [40], loss : 11.888, accuracy : 78.628%\n",
            "(squeezenet), [41], loss : 11.907, accuracy : 78.234%\n",
            "(squeezenet), [42], loss : 11.879, accuracy : 79.303%\n",
            "(squeezenet), [43], loss : 11.904, accuracy : 78.571%\n",
            "(squeezenet), [44], loss : 11.862, accuracy : 78.234%\n",
            "======================================================\n",
            "[44], TEST accuracy : 78.876%\n",
            "======================================================\n",
            "(squeezenet), [45], loss : 11.889, accuracy : 78.403%\n",
            "(squeezenet), [46], loss : 11.837, accuracy : 79.303%\n",
            "(squeezenet), [47], loss : 11.743, accuracy : 79.134%\n",
            "(squeezenet), [48], loss : 11.722, accuracy : 79.528%\n",
            "(squeezenet), [49], loss : 11.789, accuracy : 78.459%\n",
            "(squeezenet), [50], loss : 11.690, accuracy : 78.684%\n",
            "(squeezenet), [51], loss : 11.657, accuracy : 79.078%\n",
            "(squeezenet), [52], loss : 11.725, accuracy : 78.909%\n",
            "(squeezenet), [53], loss : 11.605, accuracy : 79.359%\n",
            "======================================================\n",
            "[53], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(squeezenet), [54], loss : 11.575, accuracy : 79.640%\n",
            "(squeezenet), [55], loss : 11.614, accuracy : 79.584%\n",
            "(squeezenet), [56], loss : 11.548, accuracy : 79.303%\n",
            "(squeezenet), [57], loss : 11.560, accuracy : 79.190%\n",
            "(squeezenet), [58], loss : 11.533, accuracy : 79.696%\n",
            "(squeezenet), [59], loss : 11.543, accuracy : 79.021%\n",
            "(squeezenet), [60], loss : 11.486, accuracy : 80.034%\n",
            "(squeezenet), [61], loss : 11.488, accuracy : 80.034%\n",
            "(squeezenet), [62], loss : 11.360, accuracy : 80.146%\n",
            "======================================================\n",
            "[62], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(squeezenet), [63], loss : 11.312, accuracy : 79.865%\n",
            "(squeezenet), [64], loss : 11.397, accuracy : 80.146%\n",
            "(squeezenet), [65], loss : 11.479, accuracy : 79.753%\n",
            "(squeezenet), [66], loss : 11.355, accuracy : 80.484%\n",
            "(squeezenet), [67], loss : 11.298, accuracy : 80.371%\n",
            "(squeezenet), [68], loss : 11.295, accuracy : 80.315%\n",
            "(squeezenet), [69], loss : 11.211, accuracy : 80.315%\n",
            "(squeezenet), [70], loss : 11.261, accuracy : 79.753%\n",
            "(squeezenet), [71], loss : 11.257, accuracy : 80.540%\n",
            "======================================================\n",
            "[71], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(squeezenet), [72], loss : 11.291, accuracy : 80.427%\n",
            "(squeezenet), [73], loss : 11.261, accuracy : 80.540%\n",
            "(squeezenet), [74], loss : 11.212, accuracy : 80.990%\n",
            "(squeezenet), [75], loss : 11.231, accuracy : 80.259%\n",
            "(squeezenet), [76], loss : 11.204, accuracy : 81.159%\n",
            "(squeezenet), [77], loss : 11.155, accuracy : 81.046%\n",
            "(squeezenet), [78], loss : 11.057, accuracy : 80.934%\n",
            "(squeezenet), [79], loss : 11.225, accuracy : 80.821%\n",
            "(squeezenet), [80], loss : 11.109, accuracy : 80.427%\n",
            "======================================================\n",
            "[80], TEST accuracy : 82.697%\n",
            "======================================================\n",
            "(squeezenet), [81], loss : 11.085, accuracy : 80.709%\n",
            "(squeezenet), [82], loss : 11.086, accuracy : 80.877%\n",
            "(squeezenet), [83], loss : 11.083, accuracy : 80.652%\n",
            "(squeezenet), [84], loss : 11.073, accuracy : 80.427%\n",
            "(squeezenet), [85], loss : 11.020, accuracy : 80.990%\n",
            "(squeezenet), [86], loss : 11.087, accuracy : 80.484%\n",
            "(squeezenet), [87], loss : 11.003, accuracy : 81.159%\n",
            "(squeezenet), [88], loss : 10.972, accuracy : 81.102%\n",
            "(squeezenet), [89], loss : 11.004, accuracy : 81.159%\n",
            "======================================================\n",
            "[89], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(squeezenet), [90], loss : 10.926, accuracy : 81.440%\n",
            "(squeezenet), [91], loss : 10.968, accuracy : 81.552%\n",
            "(squeezenet), [92], loss : 11.021, accuracy : 81.159%\n",
            "(squeezenet), [93], loss : 11.040, accuracy : 80.877%\n",
            "(squeezenet), [94], loss : 11.004, accuracy : 80.821%\n",
            "(squeezenet), [95], loss : 10.919, accuracy : 80.484%\n",
            "(squeezenet), [96], loss : 10.885, accuracy : 80.821%\n",
            "(squeezenet), [97], loss : 10.819, accuracy : 81.215%\n",
            "(squeezenet), [98], loss : 10.737, accuracy : 81.215%\n",
            "======================================================\n",
            "[98], TEST accuracy : 81.348%\n",
            "======================================================\n",
            "(squeezenet), [99], loss : 10.802, accuracy : 81.609%\n",
            "879856128\n",
            "879856128\n",
            "(vgg16), [0], loss : 15.830, accuracy : 69.066%\n",
            "(vgg16), [1], loss : 16.332, accuracy : 72.722%\n",
            "(vgg16), [2], loss : 15.424, accuracy : 73.622%\n",
            "(vgg16), [3], loss : 14.861, accuracy : 73.397%\n",
            "(vgg16), [4], loss : 14.502, accuracy : 74.353%\n",
            "(vgg16), [5], loss : 14.195, accuracy : 75.028%\n",
            "(vgg16), [6], loss : 14.150, accuracy : 75.591%\n",
            "(vgg16), [7], loss : 13.790, accuracy : 75.197%\n",
            "(vgg16), [8], loss : 13.584, accuracy : 76.378%\n",
            "======================================================\n",
            "[8], TEST accuracy : 74.831%\n",
            "======================================================\n",
            "(vgg16), [9], loss : 13.367, accuracy : 76.828%\n",
            "(vgg16), [10], loss : 13.443, accuracy : 77.672%\n",
            "(vgg16), [11], loss : 13.115, accuracy : 77.784%\n",
            "(vgg16), [12], loss : 13.101, accuracy : 78.796%\n",
            "(vgg16), [13], loss : 13.036, accuracy : 78.684%\n",
            "(vgg16), [14], loss : 12.959, accuracy : 78.009%\n",
            "(vgg16), [15], loss : 12.831, accuracy : 79.190%\n",
            "(vgg16), [16], loss : 12.680, accuracy : 79.359%\n",
            "(vgg16), [17], loss : 12.783, accuracy : 78.403%\n",
            "======================================================\n",
            "[17], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(vgg16), [18], loss : 12.440, accuracy : 79.415%\n",
            "(vgg16), [19], loss : 12.567, accuracy : 79.021%\n",
            "(vgg16), [20], loss : 12.415, accuracy : 79.190%\n",
            "(vgg16), [21], loss : 12.239, accuracy : 79.415%\n",
            "(vgg16), [22], loss : 12.448, accuracy : 78.909%\n",
            "(vgg16), [23], loss : 11.978, accuracy : 80.877%\n",
            "(vgg16), [24], loss : 12.218, accuracy : 80.259%\n",
            "(vgg16), [25], loss : 12.343, accuracy : 79.246%\n",
            "(vgg16), [26], loss : 12.375, accuracy : 79.584%\n",
            "======================================================\n",
            "[26], TEST accuracy : 80.225%\n",
            "======================================================\n",
            "(vgg16), [27], loss : 12.294, accuracy : 78.459%\n",
            "(vgg16), [28], loss : 12.137, accuracy : 80.765%\n",
            "(vgg16), [29], loss : 12.120, accuracy : 80.146%\n",
            "(vgg16), [30], loss : 12.238, accuracy : 79.246%\n",
            "(vgg16), [31], loss : 11.971, accuracy : 79.978%\n",
            "(vgg16), [32], loss : 11.934, accuracy : 80.259%\n",
            "(vgg16), [33], loss : 11.841, accuracy : 80.484%\n",
            "(vgg16), [34], loss : 12.106, accuracy : 80.371%\n",
            "(vgg16), [35], loss : 12.043, accuracy : 80.484%\n",
            "======================================================\n",
            "[35], TEST accuracy : 81.348%\n",
            "======================================================\n",
            "(vgg16), [36], loss : 12.142, accuracy : 79.584%\n",
            "(vgg16), [37], loss : 12.031, accuracy : 79.528%\n",
            "(vgg16), [38], loss : 11.970, accuracy : 80.259%\n",
            "(vgg16), [39], loss : 12.072, accuracy : 80.146%\n",
            "(vgg16), [40], loss : 11.893, accuracy : 80.202%\n",
            "(vgg16), [41], loss : 12.043, accuracy : 80.652%\n",
            "(vgg16), [42], loss : 12.029, accuracy : 80.371%\n",
            "(vgg16), [43], loss : 11.942, accuracy : 80.765%\n",
            "(vgg16), [44], loss : 11.718, accuracy : 81.046%\n",
            "======================================================\n",
            "[44], TEST accuracy : 80.899%\n",
            "======================================================\n",
            "(vgg16), [45], loss : 11.664, accuracy : 80.259%\n",
            "(vgg16), [46], loss : 11.789, accuracy : 80.371%\n",
            "(vgg16), [47], loss : 11.514, accuracy : 80.709%\n",
            "(vgg16), [48], loss : 11.784, accuracy : 80.034%\n",
            "(vgg16), [49], loss : 11.521, accuracy : 80.990%\n",
            "(vgg16), [50], loss : 11.586, accuracy : 81.384%\n",
            "(vgg16), [51], loss : 11.838, accuracy : 80.146%\n",
            "(vgg16), [52], loss : 11.711, accuracy : 80.484%\n",
            "(vgg16), [53], loss : 11.628, accuracy : 80.821%\n",
            "======================================================\n",
            "[53], TEST accuracy : 82.022%\n",
            "======================================================\n",
            "(vgg16), [54], loss : 11.480, accuracy : 80.540%\n",
            "(vgg16), [55], loss : 11.609, accuracy : 79.978%\n",
            "(vgg16), [56], loss : 11.794, accuracy : 79.809%\n",
            "(vgg16), [57], loss : 11.758, accuracy : 80.427%\n",
            "(vgg16), [58], loss : 11.673, accuracy : 80.877%\n",
            "(vgg16), [59], loss : 11.986, accuracy : 79.978%\n",
            "(vgg16), [60], loss : 11.730, accuracy : 79.753%\n",
            "(vgg16), [61], loss : 11.739, accuracy : 80.202%\n",
            "(vgg16), [62], loss : 11.692, accuracy : 81.046%\n",
            "======================================================\n",
            "[62], TEST accuracy : 82.247%\n",
            "======================================================\n",
            "(vgg16), [63], loss : 11.476, accuracy : 80.596%\n",
            "(vgg16), [64], loss : 11.608, accuracy : 80.540%\n",
            "(vgg16), [65], loss : 11.571, accuracy : 80.821%\n",
            "(vgg16), [66], loss : 11.557, accuracy : 80.484%\n",
            "(vgg16), [67], loss : 11.794, accuracy : 80.540%\n",
            "(vgg16), [68], loss : 11.667, accuracy : 80.090%\n",
            "(vgg16), [69], loss : 11.529, accuracy : 80.540%\n",
            "(vgg16), [70], loss : 11.507, accuracy : 81.496%\n",
            "(vgg16), [71], loss : 11.511, accuracy : 80.821%\n",
            "======================================================\n",
            "[71], TEST accuracy : 83.146%\n",
            "======================================================\n",
            "(vgg16), [72], loss : 11.595, accuracy : 80.427%\n",
            "(vgg16), [73], loss : 11.516, accuracy : 81.384%\n",
            "(vgg16), [74], loss : 11.545, accuracy : 80.315%\n",
            "(vgg16), [75], loss : 11.298, accuracy : 81.721%\n",
            "(vgg16), [76], loss : 11.460, accuracy : 80.709%\n",
            "(vgg16), [77], loss : 11.363, accuracy : 80.315%\n",
            "(vgg16), [78], loss : 11.252, accuracy : 81.384%\n",
            "(vgg16), [79], loss : 11.219, accuracy : 81.215%\n",
            "(vgg16), [80], loss : 11.365, accuracy : 81.215%\n",
            "======================================================\n",
            "[80], TEST accuracy : 79.775%\n",
            "======================================================\n",
            "(vgg16), [81], loss : 11.338, accuracy : 81.159%\n",
            "(vgg16), [82], loss : 11.247, accuracy : 81.102%\n",
            "(vgg16), [83], loss : 11.280, accuracy : 81.384%\n",
            "(vgg16), [84], loss : 11.323, accuracy : 80.877%\n",
            "(vgg16), [85], loss : 11.324, accuracy : 81.215%\n",
            "(vgg16), [86], loss : 11.506, accuracy : 81.552%\n",
            "(vgg16), [87], loss : 11.300, accuracy : 82.115%\n",
            "(vgg16), [88], loss : 11.514, accuracy : 81.215%\n",
            "(vgg16), [89], loss : 11.179, accuracy : 82.058%\n",
            "======================================================\n",
            "[89], TEST accuracy : 83.371%\n",
            "======================================================\n",
            "(vgg16), [90], loss : 11.462, accuracy : 81.102%\n",
            "(vgg16), [91], loss : 11.090, accuracy : 81.496%\n",
            "(vgg16), [92], loss : 11.207, accuracy : 80.877%\n",
            "(vgg16), [93], loss : 11.384, accuracy : 80.202%\n",
            "(vgg16), [94], loss : 11.269, accuracy : 81.271%\n",
            "(vgg16), [95], loss : 11.096, accuracy : 82.002%\n",
            "(vgg16), [96], loss : 11.060, accuracy : 81.609%\n",
            "(vgg16), [97], loss : 11.092, accuracy : 81.046%\n",
            "(vgg16), [98], loss : 11.371, accuracy : 80.990%\n",
            "======================================================\n",
            "[98], TEST accuracy : 82.022%\n",
            "======================================================\n",
            "(vgg16), [99], loss : 11.482, accuracy : 80.709%\n",
            "1365153280\n",
            "1365153280\n",
            "(densenet), [0], loss : 16.083, accuracy : 69.966%\n",
            "(densenet), [1], loss : 17.504, accuracy : 66.367%\n",
            "(densenet), [2], loss : 17.020, accuracy : 68.616%\n",
            "(densenet), [3], loss : 16.669, accuracy : 69.516%\n",
            "(densenet), [4], loss : 16.380, accuracy : 70.472%\n",
            "(densenet), [5], loss : 16.127, accuracy : 71.541%\n",
            "(densenet), [6], loss : 15.896, accuracy : 72.553%\n",
            "(densenet), [7], loss : 15.683, accuracy : 72.891%\n",
            "(densenet), [8], loss : 15.482, accuracy : 73.510%\n",
            "======================================================\n",
            "[8], TEST accuracy : 75.281%\n",
            "======================================================\n",
            "(densenet), [9], loss : 15.292, accuracy : 73.735%\n",
            "(densenet), [10], loss : 15.112, accuracy : 74.241%\n",
            "(densenet), [11], loss : 14.939, accuracy : 75.084%\n",
            "(densenet), [12], loss : 14.773, accuracy : 75.534%\n",
            "(densenet), [13], loss : 14.611, accuracy : 75.816%\n",
            "(densenet), [14], loss : 14.454, accuracy : 76.097%\n",
            "(densenet), [15], loss : 14.302, accuracy : 76.490%\n",
            "(densenet), [16], loss : 14.152, accuracy : 77.109%\n",
            "(densenet), [17], loss : 14.005, accuracy : 77.447%\n",
            "======================================================\n",
            "[17], TEST accuracy : 77.978%\n",
            "======================================================\n",
            "(densenet), [18], loss : 13.860, accuracy : 77.728%\n",
            "(densenet), [19], loss : 13.718, accuracy : 78.065%\n",
            "(densenet), [20], loss : 13.578, accuracy : 78.459%\n",
            "(densenet), [21], loss : 13.439, accuracy : 78.515%\n",
            "(densenet), [22], loss : 13.302, accuracy : 78.628%\n",
            "(densenet), [23], loss : 13.167, accuracy : 78.965%\n",
            "(densenet), [24], loss : 13.034, accuracy : 79.021%\n",
            "(densenet), [25], loss : 12.902, accuracy : 79.415%\n",
            "(densenet), [26], loss : 12.770, accuracy : 79.921%\n",
            "======================================================\n",
            "[26], TEST accuracy : 79.775%\n",
            "======================================================\n",
            "(densenet), [27], loss : 12.642, accuracy : 80.202%\n",
            "(densenet), [28], loss : 12.513, accuracy : 80.596%\n",
            "(densenet), [29], loss : 12.385, accuracy : 80.596%\n",
            "(densenet), [30], loss : 12.258, accuracy : 80.652%\n",
            "(densenet), [31], loss : 12.131, accuracy : 80.709%\n",
            "(densenet), [32], loss : 12.005, accuracy : 80.877%\n",
            "(densenet), [33], loss : 11.880, accuracy : 81.327%\n",
            "(densenet), [34], loss : 11.756, accuracy : 81.440%\n",
            "(densenet), [35], loss : 11.633, accuracy : 81.665%\n",
            "======================================================\n",
            "[35], TEST accuracy : 80.674%\n",
            "======================================================\n",
            "(densenet), [36], loss : 11.510, accuracy : 82.227%\n",
            "(densenet), [37], loss : 11.389, accuracy : 82.396%\n",
            "(densenet), [38], loss : 11.268, accuracy : 82.733%\n",
            "(densenet), [39], loss : 11.147, accuracy : 83.071%\n",
            "(densenet), [40], loss : 11.028, accuracy : 83.352%\n",
            "(densenet), [41], loss : 10.908, accuracy : 83.577%\n",
            "(densenet), [42], loss : 10.790, accuracy : 83.802%\n",
            "(densenet), [43], loss : 10.672, accuracy : 84.252%\n",
            "(densenet), [44], loss : 10.554, accuracy : 84.364%\n",
            "======================================================\n",
            "[44], TEST accuracy : 80.674%\n",
            "======================================================\n",
            "(densenet), [45], loss : 10.438, accuracy : 84.589%\n",
            "(densenet), [46], loss : 10.321, accuracy : 84.814%\n",
            "(densenet), [47], loss : 10.206, accuracy : 84.983%\n",
            "(densenet), [48], loss : 10.091, accuracy : 85.152%\n",
            "(densenet), [49], loss : 9.977, accuracy : 85.489%\n",
            "(densenet), [50], loss : 9.864, accuracy : 85.771%\n",
            "(densenet), [51], loss : 9.751, accuracy : 86.164%\n",
            "(densenet), [52], loss : 9.640, accuracy : 86.445%\n",
            "(densenet), [53], loss : 9.529, accuracy : 86.783%\n",
            "======================================================\n",
            "[53], TEST accuracy : 82.022%\n",
            "======================================================\n",
            "(densenet), [54], loss : 9.419, accuracy : 87.120%\n",
            "(densenet), [55], loss : 9.310, accuracy : 87.514%\n",
            "(densenet), [56], loss : 9.201, accuracy : 87.795%\n",
            "(densenet), [57], loss : 9.093, accuracy : 88.189%\n",
            "(densenet), [58], loss : 8.986, accuracy : 88.751%\n",
            "(densenet), [59], loss : 8.881, accuracy : 88.920%\n",
            "(densenet), [60], loss : 8.775, accuracy : 89.089%\n",
            "(densenet), [61], loss : 8.671, accuracy : 89.370%\n",
            "(densenet), [62], loss : 8.568, accuracy : 89.876%\n",
            "======================================================\n",
            "[62], TEST accuracy : 81.573%\n",
            "======================================================\n",
            "(densenet), [63], loss : 8.466, accuracy : 90.045%\n",
            "(densenet), [64], loss : 8.363, accuracy : 90.101%\n",
            "(densenet), [65], loss : 8.263, accuracy : 90.382%\n",
            "(densenet), [66], loss : 8.163, accuracy : 90.495%\n",
            "(densenet), [67], loss : 8.064, accuracy : 90.720%\n",
            "(densenet), [68], loss : 7.965, accuracy : 90.889%\n",
            "(densenet), [69], loss : 7.868, accuracy : 91.001%\n",
            "(densenet), [70], loss : 7.771, accuracy : 91.282%\n",
            "(densenet), [71], loss : 7.676, accuracy : 91.451%\n",
            "======================================================\n",
            "[71], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(densenet), [72], loss : 7.581, accuracy : 91.564%\n",
            "(densenet), [73], loss : 7.487, accuracy : 91.676%\n",
            "(densenet), [74], loss : 7.395, accuracy : 91.845%\n",
            "(densenet), [75], loss : 7.303, accuracy : 92.126%\n",
            "(densenet), [76], loss : 7.212, accuracy : 92.407%\n",
            "(densenet), [77], loss : 7.121, accuracy : 92.520%\n",
            "(densenet), [78], loss : 7.032, accuracy : 92.913%\n",
            "(densenet), [79], loss : 6.943, accuracy : 93.251%\n",
            "(densenet), [80], loss : 6.856, accuracy : 93.420%\n",
            "======================================================\n",
            "[80], TEST accuracy : 81.573%\n",
            "======================================================\n",
            "(densenet), [81], loss : 6.770, accuracy : 93.701%\n",
            "(densenet), [82], loss : 6.685, accuracy : 93.813%\n",
            "(densenet), [83], loss : 6.600, accuracy : 94.094%\n",
            "(densenet), [84], loss : 6.517, accuracy : 94.263%\n",
            "(densenet), [85], loss : 6.435, accuracy : 94.432%\n",
            "(densenet), [86], loss : 6.353, accuracy : 94.601%\n",
            "(densenet), [87], loss : 6.272, accuracy : 94.657%\n",
            "(densenet), [88], loss : 6.193, accuracy : 94.826%\n",
            "(densenet), [89], loss : 6.114, accuracy : 94.938%\n",
            "======================================================\n",
            "[89], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(densenet), [90], loss : 6.036, accuracy : 95.051%\n",
            "(densenet), [91], loss : 5.959, accuracy : 95.107%\n",
            "(densenet), [92], loss : 5.883, accuracy : 95.107%\n",
            "(densenet), [93], loss : 5.808, accuracy : 95.332%\n",
            "(densenet), [94], loss : 5.734, accuracy : 95.501%\n",
            "(densenet), [95], loss : 5.660, accuracy : 95.613%\n",
            "(densenet), [96], loss : 5.588, accuracy : 95.726%\n",
            "(densenet), [97], loss : 5.517, accuracy : 95.838%\n",
            "(densenet), [98], loss : 5.446, accuracy : 96.007%\n",
            "======================================================\n",
            "[98], TEST accuracy : 81.798%\n",
            "======================================================\n",
            "(densenet), [99], loss : 5.376, accuracy : 96.063%\n",
            "1484795904\n",
            "1484795904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "important = pd.DataFrame(index = ['accuracy'])\n",
        "for data in models:\n",
        "  name, model, acc = data\n",
        "  if acc:\n",
        "    important[name] = max(acc).item()\n",
        "\n",
        "print(important)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0J_BVpqOxNr",
        "outputId": "bfca9735-48b2-4273-dd93-48bbe684bb1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          googlenet  shufflenet  mobilenet  resnext50_32x4d  wide_resnet50_2  \\\n",
            "accuracy  84.494385   83.370789  82.471909        78.426964        78.202248   \n",
            "\n",
            "            mnasnet  resnet18    alexnet  squeezenet      vgg16   densenet  \n",
            "accuracy  76.853935  80.89888  78.651688   82.696632  83.370789  82.022476  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Cc_eUoj9Pgcu"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}