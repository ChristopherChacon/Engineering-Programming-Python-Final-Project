{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sam Ellis"
      ],
      "metadata": {
        "id": "-i9ourZXScw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Prelims"
      ],
      "metadata": {
        "id": "sSjKNNx4QVpC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi8L47EdmThs",
        "outputId": "a374365f-8c9b-4433-cc16-d04178ffcc8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a note to the reader: if you wish to run this notebook, you will need to make sure the following paths are correct. im_path must point to the elpv dataset, better_label_path must point to the train.csv, test_path must point to the test.csv, and the resnet paths must point to their corresponding parameter dictionaries. "
      ],
      "metadata": {
        "id": "6NfUHObFQgfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW97DgQPmsuR"
      },
      "outputs": [],
      "source": [
        "im_path = '/elpv-dataset'\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "\n",
        "res50_path = '/ResNetParams/resnet50.txt'\n",
        "res101_path = '/ResNetParams/resnet101.txt'\n",
        "res152_path = '/ResNetParams/resnet152.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5X0fMLloF18"
      },
      "source": [
        "## 1) Net Model Classes + Misc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple implementation of ResNet architecture that allows the users to specify in_channels and num_classes. "
      ],
      "metadata": {
        "id": "hSR6iDWsRG2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex6H5ThxoEzx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet50(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
        "\n",
        "def ResNet101(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n",
        "\n",
        "def ResNet152(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, this is a simple class for getting the dataset in a pytorch format. "
      ],
      "metadata": {
        "id": "Tkf8PLk3ROXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7HNBAw2oPOW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "class solar_cell_dataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
        "        img = Image.open(img_name)\n",
        "\n",
        "        target = torch.tensor(self.labels.iloc[idx, 1:])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhwBKkP8oaKA"
      },
      "source": [
        "## 2) Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another note to the reader: dont bother running this unless you have a beefcake GPU or colab pro. "
      ],
      "metadata": {
        "id": "d0ARjmF_RcJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDgUvT93qeYA"
      },
      "outputs": [],
      "source": [
        "######### HYPER PARAMS ###########\n",
        "batch_size = 64\n",
        "cores = 0 #-1\n",
        "lr = .001\n",
        "momentum = .6\n",
        "epochs = 100\n",
        "################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzcRIr-oVbPm",
        "outputId": "f72ef3ae-ef0f-424f-f9e4-701fd448947d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "ResNet50\n"
          ]
        }
      ],
      "source": [
        "models = [('ResNet50', ResNet50(img_channel=1, num_classes=2), res50_path),\n",
        "          ('ResNet101', ResNet101(img_channel=1, num_classes=2), res101_path),\n",
        "          ('ResNet151', ResNet152(img_channel=1, num_classes=2), res152_path)]\n",
        "          \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "name, net, params = models[0]\n",
        "net.to(device)\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I trained these networks induvidually by changing models[0] $\\Rightarrow$ models[1] or whichever one needed training. "
      ],
      "metadata": {
        "id": "feUIMKM_Rlqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ9xSY_ro0IY"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import RandomApply\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.RandomApply(nn.ModuleList([T.RandomAffine(degrees = (0, 30), translate = (0.1, 0.2), scale = (0.5, 1))]), p=.2)\n",
        "])\n",
        "\n",
        "s = solar_cell_dataset(csv_file=train_path, img_dir=im_path, transform=transforms)\n",
        "t = solar_cell_dataset(csv_file=test_path, img_dir=im_path, transform=ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(s,\n",
        "                          batch_size=batch_size,\n",
        "                          num_workers=cores)\n",
        "test_loader = DataLoader(t,\n",
        "                         batch_size=batch_size,\n",
        "                         num_workers=cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a classification function for dealing with the output of the CNN's"
      ],
      "metadata": {
        "id": "Md0ph9dtRyAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WpiaYW850iJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "s_ = nn.Softmax(dim=1)\n",
        "def classify(x):\n",
        "  x = s_(x)\n",
        "  x = torch.argmax(x, dim=1)\n",
        "  # x = F.one_hot(x, num_classes=4)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the training loop, I go for a set number of epochs but I check the test error after every 5 epochs. At the end I saved the model parameters."
      ],
      "metadata": {
        "id": "Dbe0nWTqR4p8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNoI-g-V3p_g",
        "outputId": "b499e2a5-24f6-49ad-89ec-11de53c86302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] loss : 13.901, accuracy : 71.43%, time : 25.82\n",
            "[1] loss : 14.064, accuracy : 71.88%, time : 51.16\n",
            "[2] loss : 13.894, accuracy : 71.43%, time : 76.42\n",
            "[3] loss : 13.585, accuracy : 72.50%, time : 101.87\n",
            "[4] loss : 13.542, accuracy : 72.55%, time : 127.27\n",
            "======================================================\n",
            "TEST ACCURACY : 69.89%\n",
            "======================================================\n",
            "[5] loss : 13.343, accuracy : 73.68%, time : 154.63\n",
            "[6] loss : 13.264, accuracy : 73.79%, time : 180.95\n",
            "[7] loss : 13.524, accuracy : 74.24%, time : 206.31\n",
            "[8] loss : 12.940, accuracy : 73.23%, time : 231.68\n",
            "[9] loss : 13.078, accuracy : 74.07%, time : 257.25\n",
            "======================================================\n",
            "TEST ACCURACY : 70.79%\n",
            "======================================================\n",
            "[10] loss : 12.754, accuracy : 73.57%, time : 284.68\n",
            "[11] loss : 12.630, accuracy : 74.07%, time : 309.98\n",
            "[12] loss : 12.992, accuracy : 74.63%, time : 335.24\n",
            "[13] loss : 12.444, accuracy : 75.03%, time : 360.59\n",
            "[14] loss : 12.453, accuracy : 74.92%, time : 385.83\n",
            "======================================================\n",
            "TEST ACCURACY : 72.58%\n",
            "======================================================\n",
            "[15] loss : 12.215, accuracy : 74.80%, time : 413.37\n",
            "[16] loss : 12.169, accuracy : 75.31%, time : 438.69\n",
            "[17] loss : 12.097, accuracy : 75.14%, time : 463.87\n",
            "[18] loss : 11.933, accuracy : 76.04%, time : 489.12\n",
            "[19] loss : 12.047, accuracy : 75.31%, time : 514.41\n",
            "======================================================\n",
            "TEST ACCURACY : 74.16%\n",
            "======================================================\n",
            "[20] loss : 11.744, accuracy : 76.83%, time : 541.80\n",
            "[21] loss : 11.260, accuracy : 75.82%, time : 567.13\n",
            "[22] loss : 11.374, accuracy : 77.39%, time : 592.37\n",
            "[23] loss : 11.521, accuracy : 77.11%, time : 617.51\n",
            "[24] loss : 11.030, accuracy : 76.83%, time : 642.79\n",
            "======================================================\n",
            "TEST ACCURACY : 75.28%\n",
            "======================================================\n",
            "[25] loss : 11.086, accuracy : 77.56%, time : 670.09\n",
            "[26] loss : 10.988, accuracy : 77.90%, time : 695.39\n",
            "[27] loss : 10.790, accuracy : 77.84%, time : 720.61\n",
            "[28] loss : 11.799, accuracy : 76.38%, time : 746.09\n",
            "[29] loss : 10.694, accuracy : 79.75%, time : 771.37\n",
            "======================================================\n",
            "TEST ACCURACY : 73.71%\n",
            "======================================================\n",
            "[30] loss : 10.540, accuracy : 78.68%, time : 798.89\n",
            "[31] loss : 10.591, accuracy : 79.25%, time : 824.12\n",
            "[32] loss : 10.012, accuracy : 79.30%, time : 849.34\n",
            "[33] loss : 10.242, accuracy : 80.76%, time : 874.74\n",
            "[34] loss : 9.776, accuracy : 81.33%, time : 900.21\n",
            "======================================================\n",
            "TEST ACCURACY : 74.38%\n",
            "======================================================\n",
            "[35] loss : 9.834, accuracy : 81.44%, time : 927.57\n",
            "[36] loss : 10.076, accuracy : 79.25%, time : 952.98\n",
            "[37] loss : 9.515, accuracy : 82.62%, time : 978.18\n",
            "[38] loss : 9.377, accuracy : 82.90%, time : 1003.56\n",
            "[39] loss : 10.029, accuracy : 82.28%, time : 1028.93\n",
            "======================================================\n",
            "TEST ACCURACY : 75.28%\n",
            "======================================================\n",
            "[40] loss : 9.681, accuracy : 81.55%, time : 1056.40\n",
            "[41] loss : 9.532, accuracy : 81.66%, time : 1081.65\n",
            "[42] loss : 10.238, accuracy : 79.42%, time : 1106.98\n",
            "[43] loss : 8.709, accuracy : 83.41%, time : 1132.21\n",
            "[44] loss : 9.143, accuracy : 83.01%, time : 1157.47\n",
            "======================================================\n",
            "TEST ACCURACY : 76.63%\n",
            "======================================================\n",
            "[45] loss : 8.794, accuracy : 82.68%, time : 1184.86\n",
            "[46] loss : 8.783, accuracy : 83.97%, time : 1210.10\n",
            "[47] loss : 9.315, accuracy : 83.41%, time : 1235.24\n",
            "[48] loss : 8.810, accuracy : 83.46%, time : 1260.63\n",
            "[49] loss : 8.670, accuracy : 83.30%, time : 1285.86\n",
            "======================================================\n",
            "TEST ACCURACY : 78.43%\n",
            "======================================================\n",
            "[50] loss : 8.955, accuracy : 84.87%, time : 1313.26\n",
            "[51] loss : 8.120, accuracy : 83.97%, time : 1338.54\n",
            "[52] loss : 8.617, accuracy : 84.31%, time : 1363.95\n",
            "[53] loss : 8.084, accuracy : 85.21%, time : 1389.26\n",
            "[54] loss : 7.527, accuracy : 87.06%, time : 1414.60\n",
            "======================================================\n",
            "TEST ACCURACY : 75.51%\n",
            "======================================================\n",
            "[55] loss : 7.801, accuracy : 85.32%, time : 1441.95\n",
            "[56] loss : 8.553, accuracy : 83.91%, time : 1467.23\n",
            "[57] loss : 7.985, accuracy : 84.76%, time : 1492.48\n",
            "[58] loss : 8.194, accuracy : 84.59%, time : 1517.70\n",
            "[59] loss : 7.507, accuracy : 86.33%, time : 1543.01\n",
            "======================================================\n",
            "TEST ACCURACY : 75.51%\n",
            "======================================================\n",
            "[60] loss : 10.104, accuracy : 81.61%, time : 1570.33\n",
            "[61] loss : 7.912, accuracy : 85.49%, time : 1595.76\n",
            "[62] loss : 7.530, accuracy : 87.12%, time : 1621.03\n",
            "[63] loss : 7.291, accuracy : 87.06%, time : 1646.38\n",
            "[64] loss : 7.345, accuracy : 85.66%, time : 1671.93\n",
            "======================================================\n",
            "TEST ACCURACY : 78.43%\n",
            "======================================================\n",
            "[65] loss : 9.123, accuracy : 82.85%, time : 1699.46\n",
            "[66] loss : 7.852, accuracy : 85.71%, time : 1724.86\n",
            "[67] loss : 6.976, accuracy : 87.12%, time : 1750.17\n",
            "[68] loss : 6.799, accuracy : 86.67%, time : 1775.55\n",
            "[69] loss : 8.896, accuracy : 83.30%, time : 1800.93\n",
            "======================================================\n",
            "TEST ACCURACY : 77.08%\n",
            "======================================================\n",
            "[70] loss : 6.877, accuracy : 88.08%, time : 1828.36\n",
            "[71] loss : 7.454, accuracy : 85.71%, time : 1853.69\n",
            "[72] loss : 6.337, accuracy : 88.13%, time : 1878.91\n",
            "[73] loss : 6.196, accuracy : 88.47%, time : 1904.33\n",
            "[74] loss : 6.022, accuracy : 88.98%, time : 1929.45\n",
            "======================================================\n",
            "TEST ACCURACY : 73.48%\n",
            "======================================================\n",
            "[75] loss : 6.912, accuracy : 86.56%, time : 1956.86\n",
            "[76] loss : 6.249, accuracy : 88.64%, time : 1982.39\n",
            "[77] loss : 5.876, accuracy : 89.09%, time : 2007.70\n",
            "[78] loss : 6.024, accuracy : 89.71%, time : 2032.99\n",
            "[79] loss : 5.474, accuracy : 89.60%, time : 2058.25\n",
            "======================================================\n",
            "TEST ACCURACY : 79.10%\n",
            "======================================================\n",
            "[80] loss : 6.154, accuracy : 88.25%, time : 2085.79\n",
            "[81] loss : 5.848, accuracy : 88.86%, time : 2111.15\n",
            "[82] loss : 5.838, accuracy : 89.65%, time : 2136.40\n",
            "[83] loss : 5.130, accuracy : 90.44%, time : 2161.77\n",
            "[84] loss : 5.461, accuracy : 90.61%, time : 2186.97\n",
            "======================================================\n",
            "TEST ACCURACY : 77.53%\n",
            "======================================================\n",
            "[85] loss : 4.940, accuracy : 90.27%, time : 2214.34\n",
            "[86] loss : 4.644, accuracy : 91.62%, time : 2239.64\n",
            "[87] loss : 6.373, accuracy : 88.36%, time : 2265.01\n",
            "[88] loss : 5.521, accuracy : 89.48%, time : 2290.50\n",
            "[89] loss : 5.510, accuracy : 90.27%, time : 2315.80\n",
            "======================================================\n",
            "TEST ACCURACY : 74.83%\n",
            "======================================================\n",
            "[90] loss : 3.816, accuracy : 93.08%, time : 2343.31\n",
            "[91] loss : 4.678, accuracy : 91.90%, time : 2368.75\n",
            "[92] loss : 5.018, accuracy : 90.89%, time : 2393.94\n",
            "[93] loss : 5.744, accuracy : 88.75%, time : 2419.26\n",
            "[94] loss : 4.230, accuracy : 92.24%, time : 2444.59\n",
            "======================================================\n",
            "TEST ACCURACY : 79.10%\n",
            "======================================================\n",
            "[95] loss : 3.681, accuracy : 93.76%, time : 2471.82\n",
            "[96] loss : 4.195, accuracy : 92.58%, time : 2497.30\n",
            "[97] loss : 5.111, accuracy : 90.55%, time : 2522.62\n",
            "[98] loss : 4.894, accuracy : 92.07%, time : 2547.91\n",
            "[99] loss : 3.482, accuracy : 94.26%, time : 2573.15\n",
            "======================================================\n",
            "TEST ACCURACY : 80.00%\n",
            "======================================================\n",
            "[100] loss : 3.558, accuracy : 93.98%, time : 2600.51\n",
            "[101] loss : 3.995, accuracy : 93.76%, time : 2625.92\n",
            "[102] loss : 3.189, accuracy : 95.22%, time : 2651.23\n",
            "[103] loss : 3.290, accuracy : 93.53%, time : 2676.60\n",
            "[104] loss : 3.486, accuracy : 93.53%, time : 2701.93\n",
            "======================================================\n",
            "TEST ACCURACY : 76.18%\n",
            "======================================================\n",
            "[105] loss : 2.933, accuracy : 94.94%, time : 2729.26\n",
            "[106] loss : 3.250, accuracy : 94.60%, time : 2754.51\n",
            "[107] loss : 3.309, accuracy : 95.22%, time : 2779.82\n",
            "[108] loss : 3.237, accuracy : 94.99%, time : 2805.17\n",
            "[109] loss : 4.027, accuracy : 92.52%, time : 2830.42\n",
            "======================================================\n",
            "TEST ACCURACY : 72.58%\n",
            "======================================================\n",
            "[110] loss : 3.091, accuracy : 94.83%, time : 2857.72\n",
            "[111] loss : 3.498, accuracy : 92.91%, time : 2883.05\n",
            "[112] loss : 3.788, accuracy : 93.59%, time : 2908.45\n",
            "[113] loss : 3.305, accuracy : 93.64%, time : 2933.85\n",
            "[114] loss : 2.911, accuracy : 94.66%, time : 2959.18\n",
            "======================================================\n",
            "TEST ACCURACY : 78.88%\n",
            "======================================================\n",
            "[115] loss : 2.656, accuracy : 95.56%, time : 2986.45\n",
            "[116] loss : 3.817, accuracy : 92.63%, time : 3011.70\n",
            "[117] loss : 2.973, accuracy : 95.05%, time : 3036.91\n",
            "[118] loss : 3.120, accuracy : 94.77%, time : 3062.25\n",
            "[119] loss : 2.715, accuracy : 94.94%, time : 3087.64\n",
            "======================================================\n",
            "TEST ACCURACY : 73.48%\n",
            "======================================================\n",
            "[120] loss : 2.519, accuracy : 95.78%, time : 3115.00\n",
            "[121] loss : 3.634, accuracy : 93.59%, time : 3140.27\n",
            "[122] loss : 2.725, accuracy : 95.56%, time : 3165.59\n",
            "[123] loss : 2.482, accuracy : 95.95%, time : 3190.93\n",
            "[124] loss : 3.077, accuracy : 95.22%, time : 3216.45\n",
            "======================================================\n",
            "TEST ACCURACY : 78.88%\n",
            "======================================================\n",
            "[125] loss : 3.084, accuracy : 94.09%, time : 3243.84\n",
            "[126] loss : 2.509, accuracy : 95.28%, time : 3269.11\n",
            "[127] loss : 2.602, accuracy : 95.78%, time : 3294.41\n",
            "[128] loss : 2.818, accuracy : 95.28%, time : 3319.69\n",
            "[129] loss : 3.320, accuracy : 94.38%, time : 3345.06\n",
            "======================================================\n",
            "TEST ACCURACY : 80.45%\n",
            "======================================================\n",
            "[130] loss : 2.612, accuracy : 95.22%, time : 3372.28\n",
            "[131] loss : 2.293, accuracy : 96.29%, time : 3397.74\n",
            "[132] loss : 2.065, accuracy : 96.51%, time : 3423.02\n",
            "[133] loss : 2.260, accuracy : 96.40%, time : 3448.31\n",
            "[134] loss : 2.204, accuracy : 95.89%, time : 3473.60\n",
            "======================================================\n",
            "TEST ACCURACY : 79.33%\n",
            "======================================================\n",
            "[135] loss : 2.026, accuracy : 96.51%, time : 3501.01\n",
            "[136] loss : 2.106, accuracy : 96.23%, time : 3526.41\n",
            "[137] loss : 2.147, accuracy : 96.18%, time : 3551.78\n",
            "[138] loss : 2.241, accuracy : 96.29%, time : 3576.92\n",
            "[139] loss : 1.987, accuracy : 96.79%, time : 3602.19\n",
            "======================================================\n",
            "TEST ACCURACY : 78.43%\n",
            "======================================================\n",
            "[140] loss : 2.007, accuracy : 96.40%, time : 3629.48\n",
            "[141] loss : 2.263, accuracy : 96.40%, time : 3654.62\n",
            "[142] loss : 2.557, accuracy : 95.28%, time : 3680.10\n",
            "[143] loss : 2.224, accuracy : 95.89%, time : 3705.47\n",
            "[144] loss : 2.136, accuracy : 96.29%, time : 3730.77\n",
            "======================================================\n",
            "TEST ACCURACY : 75.28%\n",
            "======================================================\n",
            "[145] loss : 2.352, accuracy : 96.12%, time : 3758.14\n",
            "[146] loss : 2.041, accuracy : 96.34%, time : 3783.47\n",
            "[147] loss : 2.282, accuracy : 95.44%, time : 3808.88\n",
            "[148] loss : 1.980, accuracy : 96.06%, time : 3834.31\n",
            "[149] loss : 1.789, accuracy : 96.46%, time : 3859.54\n",
            "======================================================\n",
            "TEST ACCURACY : 75.51%\n",
            "======================================================\n",
            "[150] loss : 2.196, accuracy : 96.46%, time : 3887.14\n",
            "[151] loss : 1.826, accuracy : 97.02%, time : 3912.57\n",
            "[152] loss : 2.472, accuracy : 96.01%, time : 3937.97\n",
            "[153] loss : 2.005, accuracy : 96.63%, time : 3963.38\n",
            "[154] loss : 2.161, accuracy : 96.40%, time : 3988.66\n",
            "======================================================\n",
            "TEST ACCURACY : 74.16%\n",
            "======================================================\n",
            "[155] loss : 2.031, accuracy : 96.46%, time : 4016.01\n",
            "[156] loss : 2.252, accuracy : 96.40%, time : 4041.28\n",
            "[157] loss : 1.868, accuracy : 96.85%, time : 4066.62\n",
            "[158] loss : 2.287, accuracy : 95.78%, time : 4091.98\n",
            "[159] loss : 2.621, accuracy : 95.22%, time : 4117.23\n",
            "======================================================\n",
            "TEST ACCURACY : 71.69%\n",
            "======================================================\n",
            "[160] loss : 2.343, accuracy : 95.84%, time : 4144.66\n",
            "[161] loss : 2.122, accuracy : 96.01%, time : 4170.10\n",
            "[162] loss : 2.045, accuracy : 96.06%, time : 4195.35\n",
            "[163] loss : 2.712, accuracy : 94.94%, time : 4220.69\n",
            "[164] loss : 2.295, accuracy : 96.34%, time : 4245.93\n",
            "======================================================\n",
            "TEST ACCURACY : 77.30%\n",
            "======================================================\n",
            "[165] loss : 1.948, accuracy : 96.46%, time : 4273.39\n",
            "[166] loss : 1.669, accuracy : 97.08%, time : 4298.61\n",
            "[167] loss : 1.801, accuracy : 96.79%, time : 4323.98\n",
            "[168] loss : 1.952, accuracy : 96.23%, time : 4349.30\n",
            "[169] loss : 1.832, accuracy : 96.68%, time : 4374.62\n",
            "======================================================\n",
            "TEST ACCURACY : 76.40%\n",
            "======================================================\n",
            "[170] loss : 1.599, accuracy : 97.02%, time : 4401.96\n",
            "[171] loss : 1.882, accuracy : 96.85%, time : 4427.40\n",
            "[172] loss : 1.749, accuracy : 96.74%, time : 4452.93\n",
            "[173] loss : 1.713, accuracy : 97.24%, time : 4478.37\n",
            "[174] loss : 1.912, accuracy : 96.57%, time : 4503.76\n",
            "======================================================\n",
            "TEST ACCURACY : 74.38%\n",
            "======================================================\n",
            "[175] loss : 1.770, accuracy : 96.63%, time : 4531.21\n",
            "[176] loss : 1.764, accuracy : 96.79%, time : 4556.66\n",
            "[177] loss : 2.093, accuracy : 96.34%, time : 4581.99\n",
            "[178] loss : 1.602, accuracy : 96.91%, time : 4607.32\n",
            "[179] loss : 1.504, accuracy : 97.41%, time : 4632.52\n",
            "======================================================\n",
            "TEST ACCURACY : 77.30%\n",
            "======================================================\n",
            "[180] loss : 1.646, accuracy : 97.02%, time : 4659.86\n",
            "[181] loss : 2.738, accuracy : 94.83%, time : 4685.24\n",
            "[182] loss : 2.030, accuracy : 96.46%, time : 4710.70\n",
            "[183] loss : 3.038, accuracy : 94.54%, time : 4736.15\n",
            "[184] loss : 2.893, accuracy : 94.66%, time : 4761.56\n",
            "======================================================\n",
            "TEST ACCURACY : 75.73%\n",
            "======================================================\n",
            "[185] loss : 2.529, accuracy : 95.44%, time : 4788.94\n",
            "[186] loss : 2.180, accuracy : 96.29%, time : 4814.20\n",
            "[187] loss : 2.617, accuracy : 95.84%, time : 4839.52\n",
            "[188] loss : 3.121, accuracy : 93.93%, time : 4864.84\n",
            "[189] loss : 2.805, accuracy : 94.21%, time : 4890.18\n",
            "======================================================\n",
            "TEST ACCURACY : 80.00%\n",
            "======================================================\n",
            "[190] loss : 1.865, accuracy : 96.63%, time : 4917.68\n",
            "[191] loss : 1.791, accuracy : 96.74%, time : 4942.82\n",
            "[192] loss : 1.917, accuracy : 96.40%, time : 4968.03\n",
            "[193] loss : 1.762, accuracy : 96.85%, time : 4993.23\n",
            "[194] loss : 1.477, accuracy : 97.13%, time : 5018.46\n",
            "======================================================\n",
            "TEST ACCURACY : 77.75%\n",
            "======================================================\n",
            "[195] loss : 1.638, accuracy : 97.08%, time : 5045.81\n",
            "[196] loss : 1.689, accuracy : 96.63%, time : 5071.29\n",
            "[197] loss : 1.877, accuracy : 96.74%, time : 5096.68\n",
            "[198] loss : 1.956, accuracy : 96.68%, time : 5121.85\n",
            "[199] loss : 1.709, accuracy : 96.63%, time : 5147.10\n",
            "======================================================\n",
            "TEST ACCURACY : 77.75%\n",
            "======================================================\n",
            "[200] loss : 1.694, accuracy : 97.19%, time : 5174.48\n",
            "[201] loss : 1.601, accuracy : 97.13%, time : 5199.82\n",
            "[202] loss : 1.555, accuracy : 97.08%, time : 5225.21\n",
            "[203] loss : 1.448, accuracy : 97.41%, time : 5250.50\n",
            "[204] loss : 1.778, accuracy : 96.96%, time : 5275.81\n",
            "======================================================\n",
            "TEST ACCURACY : 75.73%\n",
            "======================================================\n",
            "[205] loss : 2.126, accuracy : 96.12%, time : 5303.04\n",
            "[206] loss : 1.845, accuracy : 96.06%, time : 5328.38\n",
            "[207] loss : 1.833, accuracy : 96.79%, time : 5353.58\n",
            "[208] loss : 1.734, accuracy : 96.96%, time : 5378.81\n",
            "[209] loss : 1.425, accuracy : 97.53%, time : 5404.10\n",
            "======================================================\n",
            "TEST ACCURACY : 77.53%\n",
            "======================================================\n",
            "[210] loss : 1.572, accuracy : 97.02%, time : 5431.42\n",
            "[211] loss : 1.511, accuracy : 97.47%, time : 5456.62\n",
            "[212] loss : 1.945, accuracy : 96.29%, time : 5481.89\n",
            "[213] loss : 1.718, accuracy : 97.02%, time : 5507.26\n",
            "[214] loss : 2.116, accuracy : 96.23%, time : 5532.67\n",
            "======================================================\n",
            "TEST ACCURACY : 72.36%\n",
            "======================================================\n",
            "[215] loss : 1.624, accuracy : 97.58%, time : 5560.11\n",
            "[216] loss : 1.809, accuracy : 96.85%, time : 5585.37\n",
            "[217] loss : 1.510, accuracy : 97.19%, time : 5610.73\n",
            "[218] loss : 1.451, accuracy : 97.13%, time : 5635.96\n",
            "[219] loss : 1.396, accuracy : 97.19%, time : 5661.23\n",
            "======================================================\n",
            "TEST ACCURACY : 77.98%\n",
            "======================================================\n",
            "[220] loss : 1.727, accuracy : 96.96%, time : 5688.80\n",
            "[221] loss : 2.043, accuracy : 96.40%, time : 5714.15\n",
            "[222] loss : 1.597, accuracy : 96.85%, time : 5739.37\n",
            "[223] loss : 1.596, accuracy : 97.08%, time : 5764.61\n",
            "[224] loss : 2.238, accuracy : 96.23%, time : 5790.03\n",
            "======================================================\n",
            "TEST ACCURACY : 74.61%\n",
            "======================================================\n",
            "[225] loss : 2.159, accuracy : 96.18%, time : 5817.44\n",
            "[226] loss : 2.091, accuracy : 96.01%, time : 5842.74\n",
            "[227] loss : 2.241, accuracy : 96.63%, time : 5867.95\n",
            "[228] loss : 2.028, accuracy : 95.95%, time : 5893.32\n",
            "[229] loss : 2.346, accuracy : 95.78%, time : 5918.63\n",
            "======================================================\n",
            "TEST ACCURACY : 75.96%\n",
            "======================================================\n",
            "[230] loss : 1.807, accuracy : 96.57%, time : 5946.00\n",
            "[231] loss : 1.703, accuracy : 96.91%, time : 5971.28\n",
            "[232] loss : 1.485, accuracy : 97.41%, time : 5996.53\n",
            "[233] loss : 2.008, accuracy : 96.46%, time : 6021.81\n",
            "[234] loss : 1.530, accuracy : 97.53%, time : 6047.03\n",
            "======================================================\n",
            "TEST ACCURACY : 78.20%\n",
            "======================================================\n",
            "[235] loss : 1.657, accuracy : 96.91%, time : 6074.42\n",
            "[236] loss : 1.777, accuracy : 96.74%, time : 6099.67\n",
            "[237] loss : 1.513, accuracy : 96.74%, time : 6125.02\n",
            "[238] loss : 1.881, accuracy : 96.51%, time : 6150.37\n",
            "[239] loss : 1.779, accuracy : 96.74%, time : 6175.74\n",
            "======================================================\n",
            "TEST ACCURACY : 76.40%\n",
            "======================================================\n",
            "[240] loss : 1.639, accuracy : 96.85%, time : 6203.22\n",
            "[241] loss : 1.489, accuracy : 97.19%, time : 6228.52\n",
            "[242] loss : 1.852, accuracy : 96.40%, time : 6253.99\n",
            "[243] loss : 1.451, accuracy : 96.79%, time : 6279.24\n",
            "[244] loss : 1.529, accuracy : 97.24%, time : 6304.50\n",
            "======================================================\n",
            "TEST ACCURACY : 76.85%\n",
            "======================================================\n",
            "[245] loss : 1.316, accuracy : 97.53%, time : 6332.00\n",
            "[246] loss : 1.393, accuracy : 97.69%, time : 6357.31\n",
            "[247] loss : 1.257, accuracy : 97.81%, time : 6382.49\n",
            "[248] loss : 1.671, accuracy : 97.19%, time : 6407.81\n",
            "[249] loss : 2.305, accuracy : 95.89%, time : 6433.09\n",
            "======================================================\n",
            "TEST ACCURACY : 76.18%\n",
            "======================================================\n",
            "[250] loss : 3.107, accuracy : 94.09%, time : 6460.51\n",
            "[251] loss : 1.749, accuracy : 97.19%, time : 6485.81\n",
            "[252] loss : 1.419, accuracy : 97.53%, time : 6511.03\n",
            "[253] loss : 2.000, accuracy : 96.34%, time : 6536.41\n",
            "[254] loss : 1.590, accuracy : 97.13%, time : 6561.73\n",
            "======================================================\n",
            "TEST ACCURACY : 75.51%\n",
            "======================================================\n",
            "[255] loss : 1.443, accuracy : 97.47%, time : 6589.13\n",
            "[256] loss : 1.563, accuracy : 97.24%, time : 6614.40\n",
            "[257] loss : 1.340, accuracy : 97.53%, time : 6639.73\n",
            "[258] loss : 1.512, accuracy : 97.13%, time : 6665.07\n",
            "[259] loss : 1.501, accuracy : 96.91%, time : 6690.24\n",
            "======================================================\n",
            "TEST ACCURACY : 74.61%\n",
            "======================================================\n",
            "[260] loss : 1.071, accuracy : 97.75%, time : 6717.45\n",
            "[261] loss : 1.516, accuracy : 97.24%, time : 6742.83\n",
            "[262] loss : 1.677, accuracy : 97.13%, time : 6768.14\n",
            "[263] loss : 1.747, accuracy : 96.57%, time : 6793.44\n",
            "[264] loss : 1.643, accuracy : 97.24%, time : 6818.76\n",
            "======================================================\n",
            "TEST ACCURACY : 72.58%\n",
            "======================================================\n",
            "[265] loss : 1.515, accuracy : 97.64%, time : 6846.08\n",
            "[266] loss : 1.695, accuracy : 96.79%, time : 6871.36\n",
            "[267] loss : 2.192, accuracy : 95.78%, time : 6896.58\n",
            "[268] loss : 1.583, accuracy : 96.91%, time : 6921.87\n",
            "[269] loss : 2.437, accuracy : 95.73%, time : 6947.28\n",
            "======================================================\n",
            "TEST ACCURACY : 75.51%\n",
            "======================================================\n",
            "[270] loss : 2.375, accuracy : 95.67%, time : 6974.70\n",
            "[271] loss : 1.622, accuracy : 97.36%, time : 6999.87\n",
            "[272] loss : 1.692, accuracy : 96.51%, time : 7025.11\n",
            "[273] loss : 1.383, accuracy : 97.30%, time : 7050.47\n",
            "[274] loss : 1.308, accuracy : 97.75%, time : 7075.76\n",
            "======================================================\n",
            "TEST ACCURACY : 75.73%\n",
            "======================================================\n",
            "[275] loss : 1.392, accuracy : 97.58%, time : 7103.07\n",
            "[276] loss : 1.502, accuracy : 97.19%, time : 7128.28\n",
            "[277] loss : 1.682, accuracy : 97.24%, time : 7153.51\n",
            "[278] loss : 1.596, accuracy : 96.63%, time : 7178.75\n",
            "[279] loss : 1.224, accuracy : 97.75%, time : 7203.96\n",
            "======================================================\n",
            "TEST ACCURACY : 72.58%\n",
            "======================================================\n",
            "[280] loss : 1.429, accuracy : 97.36%, time : 7231.17\n",
            "[281] loss : 1.280, accuracy : 97.58%, time : 7256.52\n",
            "[282] loss : 1.317, accuracy : 97.58%, time : 7281.85\n",
            "[283] loss : 1.398, accuracy : 97.24%, time : 7307.20\n",
            "[284] loss : 1.349, accuracy : 97.41%, time : 7332.43\n",
            "======================================================\n",
            "TEST ACCURACY : 74.61%\n",
            "======================================================\n",
            "[285] loss : 1.287, accuracy : 97.86%, time : 7359.89\n",
            "[286] loss : 1.461, accuracy : 97.13%, time : 7385.16\n",
            "[287] loss : 1.657, accuracy : 97.36%, time : 7410.48\n",
            "[288] loss : 1.500, accuracy : 97.24%, time : 7435.67\n",
            "[289] loss : 1.403, accuracy : 97.86%, time : 7460.92\n",
            "======================================================\n",
            "TEST ACCURACY : 73.93%\n",
            "======================================================\n",
            "[290] loss : 1.315, accuracy : 97.98%, time : 7488.29\n",
            "[291] loss : 1.478, accuracy : 97.13%, time : 7513.59\n",
            "[292] loss : 1.694, accuracy : 97.02%, time : 7538.86\n",
            "[293] loss : 1.412, accuracy : 97.30%, time : 7564.34\n",
            "[294] loss : 1.554, accuracy : 97.02%, time : 7589.72\n",
            "======================================================\n",
            "TEST ACCURACY : 75.28%\n",
            "======================================================\n",
            "[295] loss : 1.392, accuracy : 97.69%, time : 7617.09\n",
            "[296] loss : 1.567, accuracy : 97.24%, time : 7642.43\n",
            "[297] loss : 1.469, accuracy : 97.53%, time : 7667.79\n",
            "[298] loss : 1.474, accuracy : 97.47%, time : 7693.10\n",
            "[299] loss : 1.914, accuracy : 96.68%, time : 7718.48\n",
            "======================================================\n",
            "TEST ACCURACY : 74.83%\n",
            "======================================================\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum = momentum)\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  t_loss = 0\n",
        "  acc = 0\n",
        "  for idx, data in enumerate(train_loader):\n",
        "    img, target = data\n",
        "    img = img.to(device)\n",
        "    target = target.type(torch.FloatTensor)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = net(img)\n",
        "    loss = loss_criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    t_loss += loss.item()\n",
        "    acc += (classify(output) == torch.argmax(target, dim=1)).float().sum()\n",
        "\n",
        "\n",
        "  accuracy = 100 * acc / (len(s))\n",
        "  print(f'[{epoch}] loss : {t_loss:.3f}, accuracy : {accuracy:.2f}%, time : {time.time() - start:.2f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "      for idx, data in enumerate(test_loader):\n",
        "        img, target = data\n",
        "        img = img.to(device)\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = net(img)\n",
        "        acc += (classify(output) == torch.argmax(target, dim=1)).float().sum()\n",
        "    accuracy = 100 * acc / (len(t))\n",
        "    print('======================================================')\n",
        "    print(f'TEST ACCURACY : {accuracy:.2f}%')\n",
        "    print('======================================================')\n",
        "\n",
        "# torch.save(net.state_dict(), params)\n",
        "print('DONE')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ResNets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}